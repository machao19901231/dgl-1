{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'mxnet'\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import math\n",
    "import numpy as np\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoint_chains(n_chains, length):\n",
    "    path_graph = nx.path_graph(n_chains * length).to_directed()\n",
    "    for i in range(n_chains - 1):  # break the path graph into N chains\n",
    "        path_graph.remove_edge((i + 1) * length - 1, (i + 1) * length)\n",
    "        path_graph.remove_edge((i + 1) * length, (i + 1) * length - 1)\n",
    "    for n in path_graph.nodes:\n",
    "        path_graph.add_edge(n, n)  # add self connections\n",
    "    return path_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = disjoint_chains(1, 30)\n",
    "nx.draw(g1, pos=nx.circular_layout(g1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g2 = disjoint_chains(2, 15)\n",
    "nx.draw(g2, pos=nx.circular_layout(g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 5000\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "\n",
    "g = dgl.DGLGraph(disjoint_chains(1, 10000), readonly=True)\n",
    "labels = mx.nd.zeros([g.number_of_nodes()])\n",
    "#pattern = [0, 1] # This can be classified.\n",
    "#pattern = [0, 1, 1, 1] # This can be classified.\n",
    "pattern = [0, 1, 0, 1, 1, 0] # This can't be classified.\n",
    "#pattern = [0, 1, 1, 0, 1, 1, 1, 0] # This can be classified.\n",
    "for i in range(int(g.number_of_nodes()/len(pattern))):\n",
    "    labels[i*len(pattern):(i + 1) * len(pattern)] = pattern\n",
    "\n",
    "train_mask = mx.nd.zeros((g.number_of_nodes()))\n",
    "train_mask[np.random.randint(0, g.number_of_nodes(), size=n_train_samples)] = 1\n",
    "eval_mask = 1 - train_mask\n",
    "n_train_samples = mx.nd.sum(train_mask).asnumpy()[0]\n",
    "\n",
    "def evaluate(pred, labels, mask):\n",
    "    pred = pred.argmax(axis=1)\n",
    "    accuracy = ((pred == labels) * mask).sum() / mask.sum().asscalar()\n",
    "    return accuracy.asscalar()\n",
    "\n",
    "# Helper function to convert a number \n",
    "# to its fixed width binary representation\n",
    "def conv(x):\n",
    "  a = format(x, '032b')\n",
    "  l = list(str(a))\n",
    "  l = np.array(list(map(int, l)))\n",
    "  return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we predict with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 16\n",
    "n_classes = 2\n",
    "\n",
    "class MLP(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "        self.dense2 = gluon.nn.Dense(n_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        return self.dense2(self.dense1(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.00574109], acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = MLP(n_hidden, n_classes, 'relu')\n",
    "model.initialize()\n",
    "features = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(1000):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "print(\"loss: \" + str(loss.asnumpy()) + \", acc: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.00170058], acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def rotate(feats, i):\n",
    "    if i == 0:\n",
    "        return feats\n",
    "    \n",
    "    data = mx.nd.zeros(feats.shape)\n",
    "    data[0:i] = feats[(data.shape[0] - i):]\n",
    "    data[i:] = feats[0:(data.shape[0] - i)]\n",
    "    return data\n",
    "    \n",
    "model = MLP(n_hidden, n_classes, 'relu')\n",
    "model.initialize()\n",
    "base_feats = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "features = [rotate(base_feats, i) for i in range(len(pattern))]\n",
    "features = mx.nd.concat(*features, dim=1)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(1000):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "print(\"loss: \" + str(loss.asnumpy()) + \", acc: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we define a GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 16\n",
    "n_classes = 2\n",
    "\n",
    "class GCNLayer(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 activation):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.dense = gluon.nn.Dense(out_feats, activation=activation)\n",
    "\n",
    "    def forward(self, h):\n",
    "        self.g.ndata['h'] = h\n",
    "        def concat_msg(edges):\n",
    "            return {'m': edges.src['h']}\n",
    "        def red_func(nodes):\n",
    "            m = nodes.mailbox['m']\n",
    "            if m.shape[1] == 3:\n",
    "                h = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(h, nodes.data['h'], dim=1)\n",
    "            else:\n",
    "                num_feats = m.shape[2]\n",
    "                m = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(m, nodes.data['h'], mx.nd.zeros(shape=(m.shape[0], num_feats)), dim=1)\n",
    "            return {'h': self.dense(h)}\n",
    "        self.g.update_all(concat_msg, red_func)\n",
    "        return mx.nd.concat(self.g.ndata.pop('h'), h, dim=1)\n",
    "\n",
    "class GCN(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = gluon.nn.Sequential()\n",
    "        # input layer\n",
    "        self.layers.add(GCNLayer(g, in_feats, n_hidden, activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.add(GCNLayer(g, n_hidden, n_hidden, activation))\n",
    "        self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "        self.dense2 = gluon.nn.Dense(n_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        return self.dense2(self.dense1(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dzzhen/Workspace/dgl/python/dgl/base.py:17: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5909139\n"
     ]
    }
   ],
   "source": [
    "degs = g.in_degrees().astype('float32')\n",
    "norm = mx.nd.power(degs, -0.5)\n",
    "g.ndata['norm'] = mx.nd.expand_dims(norm, 1)\n",
    "features = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "model = GCN(g, in_feats=features.shape[1], n_hidden=16, n_classes=2, n_layers=3, activation='relu', dropout=0.5)\n",
    "model.initialize()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "            {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "h = model(features)\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(100):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here we define an SSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteadyStateOperator(gluon.Block):\n",
    "    def __init__(self, n_hidden, activation, **kwargs):\n",
    "        super(SteadyStateOperator, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "            self.dense2 = gluon.nn.Dense(n_hidden)\n",
    "\n",
    "    def forward(self, g):\n",
    "        def concat_msg(edges):\n",
    "            return {'m': edges.src['xh']}\n",
    "        def red_func(nodes):\n",
    "            m = nodes.mailbox['m']\n",
    "            if m.shape[1] == 3:\n",
    "                h = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(h, nodes.data['h'], dim=1)\n",
    "            else:\n",
    "                num_feats = m.shape[2]\n",
    "                m = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(m, nodes.data['h'], mx.nd.zeros(shape=(m.shape[0], num_feats)), dim=1)\n",
    "            return {'h': self.dense2(self.dense1(h))}\n",
    "        \n",
    "        g.ndata['xh'] = mx.nd.concat(g.ndata['x'], g.ndata['h'], dim=1)\n",
    "        g.update_all(concat_msg, red_func)\n",
    "        return g.ndata['h']\n",
    "\n",
    "def update_embeddings(g, steady_state_operator):\n",
    "    prev_h = g.ndata['h']\n",
    "    next_h = steady_state_operator(g)\n",
    "    g.ndata['h'] = (1 - alpha) * prev_h + alpha * next_h\n",
    "    \n",
    "class Predictor(gluon.Block):\n",
    "    def __init__(self, n_hidden, activation, **kwargs):\n",
    "        super(Predictor, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "            self.dense2 = gluon.nn.Dense(2)  ## binary classifier\n",
    "\n",
    "    def forward(self, g):\n",
    "        g.ndata['z'] = self.dense2(self.dense1(g.ndata['h']))\n",
    "        return g.ndata['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dzzhen/Workspace/dgl/python/dgl/base.py:17: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.6931569]\n",
      "0.4930853\n",
      "loss: [0.69690686]\n",
      "0.49802437\n",
      "loss: [0.72387487]\n",
      "0.50197566\n",
      "loss: [0.69428223]\n",
      "0.50197566\n",
      "loss: [0.6933636]\n",
      "0.49802437\n",
      "loss: [0.69891757]\n",
      "0.49802437\n",
      "loss: [0.69880486]\n",
      "0.49802437\n",
      "loss: [0.6971408]\n",
      "0.49802437\n",
      "loss: [0.6941411]\n",
      "0.49802437\n",
      "loss: [0.6913432]\n",
      "0.49802437\n",
      "loss: [0.6904655]\n",
      "0.51284164\n",
      "loss: [0.6906769]\n",
      "0.50197566\n",
      "loss: [0.68948287]\n",
      "0.5301284\n",
      "loss: [0.687774]\n",
      "0.5935133\n",
      "loss: [0.6865534]\n",
      "0.5615739\n",
      "loss: [0.6841595]\n",
      "0.57342774\n",
      "loss: [0.6816937]\n",
      "0.59976953\n",
      "loss: [0.679896]\n",
      "0.59812313\n",
      "loss: [0.6767465]\n",
      "0.60190976\n",
      "loss: [0.6752209]\n",
      "0.59680605\n",
      "loss: [0.67245495]\n",
      "0.60487324\n",
      "loss: [0.67028856]\n",
      "0.61228186\n",
      "loss: [0.6689542]\n",
      "0.6070135\n",
      "loss: [0.6658506]\n",
      "0.62314785\n",
      "loss: [0.66488844]\n",
      "0.62232465\n",
      "loss: [0.6636536]\n",
      "0.6244649\n",
      "loss: [0.66139287]\n",
      "0.6351663\n",
      "loss: [0.6607401]\n",
      "0.63697726\n",
      "loss: [0.6610206]\n",
      "0.6302272\n",
      "loss: [0.6599398]\n",
      "0.6404346\n",
      "loss: [0.65786797]\n",
      "0.6483372\n",
      "loss: [0.65722346]\n",
      "0.6516299\n",
      "loss: [0.6581288]\n",
      "0.64866644\n",
      "loss: [0.6585834]\n",
      "0.64290416\n",
      "loss: [0.65692186]\n",
      "0.65409946\n",
      "loss: [0.65564626]\n",
      "0.65689826\n",
      "loss: [0.6565253]\n",
      "0.649325\n",
      "loss: [0.6584309]\n",
      "0.64866644\n",
      "loss: [0.6585875]\n",
      "0.63697726\n",
      "loss: [0.6558354]\n",
      "0.65311164\n",
      "loss: [0.6551413]\n",
      "0.6564044\n",
      "loss: [0.65700305]\n",
      "0.642081\n",
      "loss: [0.65627456]\n",
      "0.65179455\n",
      "loss: [0.65442723]\n",
      "0.65508723\n",
      "loss: [0.65545267]\n",
      "0.651136\n",
      "loss: [0.6567389]\n",
      "0.64948964\n",
      "loss: [0.65590054]\n",
      "0.649325\n",
      "loss: [0.6543426]\n",
      "0.6595324\n",
      "loss: [0.65479296]\n",
      "0.6575568\n",
      "loss: [0.6559646]\n",
      "0.649325\n",
      "loss: [0.6553841]\n",
      "0.6564044\n",
      "loss: [0.65421695]\n",
      "0.65591043\n",
      "loss: [0.6546031]\n",
      "0.65311164\n",
      "loss: [0.65544575]\n",
      "0.6544287\n",
      "loss: [0.6550382]\n",
      "0.6513006\n",
      "loss: [0.65406394]\n",
      "0.6588739\n",
      "loss: [0.654112]\n",
      "0.6600264\n",
      "loss: [0.65486753]\n",
      "0.6534409\n",
      "loss: [0.65514684]\n",
      "0.656569\n",
      "loss: [0.65455276]\n",
      "0.65327626\n",
      "loss: [0.6537476]\n",
      "0.66134346\n",
      "loss: [0.6536841]\n",
      "0.66134346\n",
      "loss: [0.65417075]\n",
      "0.6539348\n",
      "loss: [0.65446407]\n",
      "0.65821534\n",
      "loss: [0.6542838]\n",
      "0.65228844\n",
      "loss: [0.653729]\n",
      "0.6598617\n",
      "loss: [0.6533327]\n",
      "0.6587092\n",
      "loss: [0.65328676]\n",
      "0.6587092\n",
      "loss: [0.65352154]\n",
      "0.6595324\n",
      "loss: [0.6539654]\n",
      "0.6536055\n",
      "loss: [0.6543977]\n",
      "0.6575568\n",
      "loss: [0.65470207]\n",
      "0.65097135\n",
      "loss: [0.654555]\n",
      "0.65673363\n",
      "loss: [0.6540653]\n",
      "0.65228844\n",
      "loss: [0.6534323]\n",
      "0.66183734\n",
      "loss: [0.65316397]\n",
      "0.660191\n",
      "loss: [0.6532822]\n",
      "0.6593678\n",
      "loss: [0.65362793]\n",
      "0.6606849\n",
      "loss: [0.65405726]\n",
      "0.65228844\n",
      "loss: [0.654342]\n",
      "0.6578861\n",
      "loss: [0.65441626]\n",
      "0.65146524\n",
      "loss: [0.6540451]\n",
      "0.65772146\n",
      "loss: [0.6535474]\n",
      "0.6557458\n",
      "loss: [0.65314126]\n",
      "0.66052026\n",
      "loss: [0.6530678]\n",
      "0.6603556\n",
      "loss: [0.65327734]\n",
      "0.6580507\n",
      "loss: [0.6537435]\n",
      "0.66052026\n",
      "loss: [0.6544711]\n",
      "0.65179455\n",
      "loss: [0.6550858]\n",
      "0.6542641\n",
      "loss: [0.65526646]\n",
      "0.6499835\n",
      "loss: [0.6546427]\n",
      "0.65607506\n",
      "loss: [0.6536422]\n",
      "0.6549226\n",
      "loss: [0.6530253]\n",
      "0.66052026\n",
      "loss: [0.65323967]\n",
      "0.6616727\n",
      "loss: [0.6538945]\n",
      "0.65311164\n",
      "loss: [0.65419716]\n",
      "0.6588739\n",
      "loss: [0.6539039]\n",
      "0.652947\n",
      "loss: [0.6532771]\n",
      "0.66117877\n",
      "loss: [0.65299535]\n",
      "0.6593678\n",
      "loss: [0.6531934]\n",
      "0.65903854\n",
      "loss: [0.65354]\n",
      "0.6600264\n",
      "loss: [0.6536942]\n",
      "0.654758\n",
      "loss: [0.65343064]\n",
      "0.6598617\n",
      "loss: [0.65310955]\n",
      "0.65969706\n",
      "loss: [0.65296704]\n",
      "0.6592032\n",
      "loss: [0.6530838]\n",
      "0.6608495\n",
      "loss: [0.6533076]\n",
      "0.65673363\n",
      "loss: [0.6533807]\n",
      "0.66052026\n",
      "loss: [0.65331364]\n",
      "0.6564044\n",
      "loss: [0.6531033]\n",
      "0.66134346\n",
      "loss: [0.6529564]\n",
      "0.6600264\n",
      "loss: [0.652915]\n",
      "0.660191\n",
      "loss: [0.6529777]\n",
      "0.6606849\n",
      "loss: [0.6531061]\n",
      "0.6593678\n",
      "loss: [0.65318674]\n",
      "0.6621666\n",
      "loss: [0.6532597]\n",
      "0.65689826\n",
      "loss: [0.6532395]\n",
      "0.66052026\n",
      "loss: [0.6532307]\n",
      "0.65689826\n",
      "loss: [0.6531472]\n",
      "0.6621666\n",
      "loss: [0.65308756]\n",
      "0.6592032\n",
      "loss: [0.6530009]\n",
      "0.66134346\n",
      "loss: [0.65295464]\n",
      "0.660191\n",
      "loss: [0.6528981]\n",
      "0.66101414\n",
      "loss: [0.65287346]\n",
      "0.6603556\n",
      "loss: [0.65284175]\n",
      "0.66117877\n",
      "loss: [0.65282553]\n",
      "0.6600264\n",
      "loss: [0.6528014]\n",
      "0.66134346\n",
      "loss: [0.65278053]\n",
      "0.65969706\n",
      "loss: [0.65275294]\n",
      "0.660191\n",
      "loss: [0.65271866]\n",
      "0.6598617\n",
      "loss: [0.6526755]\n",
      "0.6600264\n",
      "loss: [0.652598]\n",
      "0.66052026\n",
      "loss: [0.6524702]\n",
      "0.65969706\n",
      "loss: [0.6523865]\n",
      "0.6616727\n",
      "loss: [0.652537]\n",
      "0.660191\n",
      "loss: [0.65261525]\n",
      "0.6606849\n",
      "loss: [0.65241116]\n",
      "0.6598617\n",
      "loss: [0.6521492]\n",
      "0.66101414\n",
      "loss: [0.6521598]\n",
      "0.6606849\n",
      "loss: [0.652171]\n",
      "0.66200197\n",
      "loss: [0.6521241]\n",
      "0.66134346\n",
      "loss: [0.6520157]\n",
      "0.6616727\n",
      "loss: [0.6520156]\n",
      "0.65903854\n",
      "loss: [0.6522522]\n",
      "0.66134346\n",
      "loss: [0.6524798]\n",
      "0.6562397\n",
      "loss: [0.652662]\n",
      "0.6575568\n",
      "loss: [0.6531008]\n",
      "0.6536055\n",
      "loss: [0.654113]\n",
      "0.6536055\n",
      "loss: [0.6550268]\n",
      "0.6476786\n",
      "loss: [0.656092]\n",
      "0.64455056\n",
      "loss: [0.6548661]\n",
      "0.64883107\n",
      "loss: [0.65270907]\n",
      "0.656569\n",
      "loss: [0.65162015]\n",
      "0.66101414\n",
      "loss: [0.6532]\n",
      "0.65179455\n",
      "loss: [0.65528476]\n",
      "0.64948964\n",
      "loss: [0.6548568]\n",
      "0.6485018\n",
      "loss: [0.65255386]\n",
      "0.6587092\n",
      "loss: [0.6516957]\n",
      "0.6623312\n",
      "loss: [0.65330046]\n",
      "0.65228844\n",
      "loss: [0.6539296]\n",
      "0.65327626\n",
      "loss: [0.65209484]\n",
      "0.6580507\n",
      "loss: [0.6522794]\n",
      "0.6557458\n",
      "loss: [0.65393984]\n",
      "0.6544287\n",
      "loss: [0.65323853]\n",
      "0.6527824\n",
      "loss: [0.6517995]\n",
      "0.6615081\n",
      "loss: [0.65234]\n",
      "0.65969706\n",
      "loss: [0.6530945]\n",
      "0.6526177\n",
      "loss: [0.65224594]\n",
      "0.6606849\n",
      "loss: [0.65185815]\n",
      "0.66134346\n",
      "loss: [0.6526808]\n",
      "0.65459335\n",
      "loss: [0.65251595]\n",
      "0.65838\n",
      "loss: [0.6517818]\n",
      "0.6595324\n",
      "loss: [0.6520079]\n",
      "0.65821534\n",
      "loss: [0.6524449]\n",
      "0.65821534\n",
      "loss: [0.6519473]\n",
      "0.65903854\n",
      "loss: [0.6518511]\n",
      "0.6598617\n",
      "loss: [0.6522732]\n",
      "0.6600264\n",
      "loss: [0.65201247]\n",
      "0.65739214\n",
      "loss: [0.65163463]\n",
      "0.6598617\n",
      "loss: [0.6519092]\n",
      "0.66134346\n",
      "loss: [0.65205663]\n",
      "0.6575568\n",
      "loss: [0.65178394]\n",
      "0.6615081\n",
      "loss: [0.6515915]\n",
      "0.66052026\n",
      "loss: [0.6517515]\n",
      "0.6595324\n",
      "loss: [0.6517556]\n",
      "0.6621666\n",
      "loss: [0.65167314]\n",
      "0.6598617\n",
      "loss: [0.65155965]\n",
      "0.6608495\n",
      "loss: [0.6515272]\n",
      "0.66183734\n",
      "loss: [0.6515516]\n",
      "0.660191\n",
      "loss: [0.65161234]\n",
      "0.6616727\n",
      "loss: [0.65151596]\n",
      "0.660191\n",
      "loss: [0.65142626]\n",
      "0.660191\n",
      "loss: [0.65161437]\n",
      "0.6615081\n",
      "loss: [0.651713]\n",
      "0.6588739\n",
      "loss: [0.65156066]\n",
      "0.66183734\n",
      "loss: [0.6513867]\n",
      "0.6603556\n",
      "loss: [0.6514087]\n",
      "0.6603556\n",
      "loss: [0.6513768]\n",
      "0.6616727\n",
      "loss: [0.6514805]\n",
      "0.660191\n",
      "loss: [0.6515985]\n",
      "0.66183734\n",
      "loss: [0.6515475]\n",
      "0.6595324\n",
      "loss: [0.6513998]\n",
      "0.66117877\n",
      "loss: [0.65134126]\n",
      "0.6606849\n",
      "loss: [0.65131146]\n",
      "0.6600264\n",
      "loss: [0.6512489]\n",
      "0.6603556\n",
      "loss: [0.65127754]\n",
      "0.66052026\n",
      "loss: [0.6512726]\n",
      "0.6603556\n",
      "loss: [0.6512067]\n",
      "0.66052026\n",
      "loss: [0.65118253]\n",
      "0.6606849\n",
      "loss: [0.65121394]\n",
      "0.6603556\n",
      "loss: [0.6511666]\n",
      "0.6606849\n",
      "loss: [0.65123385]\n",
      "0.6621666\n",
      "loss: [0.6512899]\n",
      "0.6603556\n",
      "loss: [0.6513216]\n",
      "0.6621666\n",
      "loss: [0.6514265]\n",
      "0.65903854\n",
      "loss: [0.6515734]\n",
      "0.66117877\n",
      "loss: [0.6514388]\n",
      "0.6587092\n",
      "loss: [0.6513286]\n",
      "0.66183734\n",
      "loss: [0.6512848]\n",
      "0.660191\n",
      "loss: [0.6512575]\n",
      "0.6623312\n",
      "loss: [0.6511961]\n",
      "0.6606849\n",
      "loss: [0.65112084]\n",
      "0.6621666\n",
      "loss: [0.6511134]\n",
      "0.6606849\n",
      "loss: [0.65109587]\n",
      "0.66134346\n",
      "loss: [0.65109235]\n",
      "0.6603556\n",
      "loss: [0.651109]\n",
      "0.66183734\n",
      "loss: [0.65111524]\n",
      "0.6603556\n",
      "loss: [0.65111196]\n",
      "0.66183734\n",
      "loss: [0.65120673]\n",
      "0.6600264\n",
      "loss: [0.6512936]\n",
      "0.6621666\n",
      "loss: [0.6514103]\n",
      "0.65903854\n",
      "loss: [0.6517369]\n",
      "0.65969706\n",
      "loss: [0.65234834]\n",
      "0.65459335\n",
      "loss: [0.65298426]\n",
      "0.6542641\n",
      "loss: [0.6527994]\n",
      "0.6521238\n",
      "loss: [0.65242565]\n",
      "0.65591043\n",
      "loss: [0.6514165]\n",
      "0.65903854\n",
      "loss: [0.65120894]\n",
      "0.6598617\n",
      "loss: [0.6524347]\n",
      "0.65689826\n",
      "loss: [0.65375537]\n",
      "0.6498189\n",
      "loss: [0.6537607]\n",
      "0.65311164\n",
      "loss: [0.6524406]\n",
      "0.65459335\n",
      "loss: [0.6512255]\n",
      "0.66183734\n",
      "loss: [0.6512624]\n",
      "0.6615081\n",
      "loss: [0.65191686]\n",
      "0.65607506\n",
      "loss: [0.6520903]\n",
      "0.6572275\n",
      "loss: [0.6513032]\n",
      "0.6593678\n",
      "loss: [0.651114]\n",
      "0.6606849\n",
      "loss: [0.6516097]\n",
      "0.6593678\n",
      "loss: [0.65168655]\n",
      "0.6572275\n",
      "loss: [0.6512857]\n",
      "0.6600264\n",
      "loss: [0.6509696]\n",
      "0.6598617\n",
      "loss: [0.65101826]\n",
      "0.6600264\n",
      "loss: [0.6513705]\n",
      "0.660191\n",
      "loss: [0.65143186]\n",
      "0.6572275\n",
      "loss: [0.65107244]\n",
      "0.66200197\n",
      "loss: [0.65083355]\n",
      "0.66052026\n",
      "loss: [0.65101695]\n",
      "0.660191\n",
      "loss: [0.6513575]\n",
      "0.65969706\n",
      "loss: [0.65133625]\n",
      "0.65821534\n",
      "loss: [0.65098816]\n",
      "0.66183734\n",
      "loss: [0.65076256]\n",
      "0.66052026\n",
      "loss: [0.650988]\n",
      "0.6595324\n",
      "loss: [0.6512963]\n",
      "0.65969706\n",
      "loss: [0.651057]\n",
      "0.6593678\n",
      "loss: [0.6506882]\n",
      "0.66134346\n",
      "loss: [0.6507085]\n",
      "0.66183734\n",
      "loss: [0.6508579]\n",
      "0.6595324\n",
      "loss: [0.65087134]\n",
      "0.66117877\n",
      "loss: [0.6507566]\n",
      "0.6598617\n",
      "loss: [0.65059346]\n",
      "0.6624959\n",
      "loss: [0.65054065]\n",
      "0.6616727\n",
      "loss: [0.65063184]\n",
      "0.6595324\n",
      "loss: [0.65066975]\n",
      "0.6624959\n",
      "loss: [0.6506766]\n",
      "0.65969706\n",
      "loss: [0.6505326]\n",
      "0.6623312\n",
      "loss: [0.65047586]\n",
      "0.66117877\n",
      "loss: [0.65052444]\n",
      "0.6608495\n",
      "loss: [0.650663]\n",
      "0.66183734\n",
      "loss: [0.6508677]\n",
      "0.6595324\n",
      "loss: [0.65093285]\n",
      "0.65969706\n",
      "loss: [0.6510144]\n",
      "0.65903854\n",
      "loss: [0.6509363]\n",
      "0.65969706\n",
      "loss: [0.65090024]\n",
      "0.6598617\n",
      "loss: [0.650721]\n",
      "0.6608495\n",
      "loss: [0.65063477]\n",
      "0.6598617\n",
      "loss: [0.65052736]\n",
      "0.66282517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.6504789]\n",
      "0.66101414\n",
      "loss: [0.65041745]\n",
      "0.66134346\n",
      "loss: [0.6504233]\n",
      "0.66134346\n",
      "loss: [0.65044934]\n",
      "0.66117877\n",
      "loss: [0.65050656]\n",
      "0.6624959\n",
      "loss: [0.6507195]\n",
      "0.6587092\n",
      "loss: [0.65083325]\n",
      "0.6600264\n",
      "loss: [0.6508524]\n",
      "0.6585446\n",
      "loss: [0.6508909]\n",
      "0.6600264\n",
      "loss: [0.6510417]\n",
      "0.6580507\n",
      "loss: [0.65086824]\n",
      "0.6593678\n",
      "loss: [0.65078115]\n",
      "0.6595324\n",
      "loss: [0.650621]\n",
      "0.66183734\n",
      "loss: [0.65044874]\n",
      "0.6603556\n",
      "loss: [0.6503683]\n",
      "0.66134346\n",
      "loss: [0.65039873]\n",
      "0.6624959\n",
      "loss: [0.65046453]\n",
      "0.6603556\n",
      "loss: [0.65053505]\n",
      "0.6621666\n",
      "loss: [0.65072805]\n",
      "0.6585446\n",
      "loss: [0.6507711]\n",
      "0.66052026\n",
      "loss: [0.650709]\n",
      "0.6587092\n",
      "loss: [0.6505907]\n",
      "0.6608495\n",
      "loss: [0.65047127]\n",
      "0.6598617\n",
      "loss: [0.65036416]\n",
      "0.6629898\n",
      "loss: [0.6503183]\n",
      "0.66101414\n",
      "loss: [0.65040517]\n",
      "0.66101414\n",
      "loss: [0.6506407]\n",
      "0.66183734\n",
      "loss: [0.65104353]\n",
      "0.65903854\n",
      "loss: [0.65129894]\n",
      "0.6587092\n",
      "loss: [0.6516115]\n",
      "0.6549226\n",
      "loss: [0.6514921]\n",
      "0.6562397\n",
      "loss: [0.6513734]\n",
      "0.6562397\n",
      "loss: [0.6508185]\n",
      "0.65969706\n",
      "loss: [0.6504475]\n",
      "0.6615081\n",
      "loss: [0.6503621]\n",
      "0.66052026\n",
      "loss: [0.65051526]\n",
      "0.6623312\n",
      "loss: [0.65089715]\n",
      "0.6587092\n",
      "loss: [0.651134]\n",
      "0.6592032\n",
      "loss: [0.65098685]\n",
      "0.65821534\n",
      "loss: [0.6507371]\n",
      "0.660191\n",
      "loss: [0.6505448]\n",
      "0.6603556\n",
      "loss: [0.6504217]\n",
      "0.6608495\n",
      "loss: [0.6504889]\n",
      "0.6624959\n",
      "loss: [0.6506419]\n",
      "0.6600264\n",
      "loss: [0.65063494]\n",
      "0.6616727\n",
      "loss: [0.65054905]\n",
      "0.660191\n",
      "loss: [0.6504366]\n",
      "0.66282517\n",
      "loss: [0.65040356]\n",
      "0.6606849\n",
      "loss: [0.6503679]\n",
      "0.66117877\n",
      "loss: [0.6503802]\n",
      "0.66200197\n",
      "loss: [0.6503687]\n",
      "0.6606849\n",
      "loss: [0.6504288]\n",
      "0.6629898\n",
      "loss: [0.6504582]\n",
      "0.6606849\n",
      "loss: [0.6504543]\n",
      "0.6621666\n",
      "loss: [0.6504422]\n",
      "0.6600264\n",
      "loss: [0.6504547]\n",
      "0.66183734\n",
      "loss: [0.6506034]\n",
      "0.6598617\n",
      "loss: [0.6508068]\n",
      "0.6593678\n",
      "loss: [0.6510144]\n",
      "0.6580507\n",
      "loss: [0.65115166]\n",
      "0.6578861\n",
      "loss: [0.6510429]\n",
      "0.65838\n",
      "loss: [0.6506753]\n",
      "0.6608495\n",
      "loss: [0.65037894]\n",
      "0.66101414\n",
      "loss: [0.6503403]\n",
      "0.66134346\n",
      "loss: [0.6506734]\n",
      "0.66101414\n",
      "loss: [0.6513366]\n",
      "0.65607506\n",
      "loss: [0.6515928]\n",
      "0.65607506\n",
      "loss: [0.6515063]\n",
      "0.65459335\n",
      "loss: [0.651041]\n",
      "0.6592032\n",
      "loss: [0.6504395]\n",
      "0.66052026\n",
      "loss: [0.65048665]\n",
      "0.6603556\n",
      "loss: [0.65122306]\n",
      "0.65739214\n",
      "loss: [0.65178156]\n",
      "0.65409946\n",
      "loss: [0.6513017]\n",
      "0.65739214\n",
      "loss: [0.6505668]\n",
      "0.65969706\n",
      "loss: [0.6503752]\n",
      "0.6608495\n",
      "loss: [0.65071553]\n",
      "0.66052026\n",
      "loss: [0.6511351]\n",
      "0.656569\n",
      "loss: [0.6512874]\n",
      "0.6580507\n",
      "loss: [0.6508172]\n",
      "0.6593678\n",
      "loss: [0.6504348]\n",
      "0.6621666\n",
      "loss: [0.6504635]\n",
      "0.6624959\n",
      "loss: [0.6508432]\n",
      "0.6592032\n",
      "loss: [0.65111625]\n",
      "0.6593678\n",
      "loss: [0.65100855]\n",
      "0.65772146\n",
      "loss: [0.65061307]\n",
      "0.6608495\n",
      "loss: [0.65041095]\n",
      "0.660191\n",
      "loss: [0.65054655]\n",
      "0.6608495\n",
      "loss: [0.65075874]\n",
      "0.66052026\n",
      "loss: [0.6508608]\n",
      "0.65838\n",
      "loss: [0.6506299]\n",
      "0.66117877\n",
      "loss: [0.650418]\n",
      "0.6608495\n",
      "loss: [0.6503918]\n",
      "0.6606849\n",
      "loss: [0.6505707]\n",
      "0.6616727\n",
      "loss: [0.6507998]\n",
      "0.6592032\n",
      "loss: [0.65079856]\n",
      "0.6598617\n",
      "loss: [0.6505215]\n",
      "0.6603556\n",
      "loss: [0.65036905]\n",
      "0.6606849\n",
      "loss: [0.65058386]\n",
      "0.66134346\n",
      "loss: [0.6511075]\n",
      "0.65821534\n",
      "loss: [0.6510805]\n",
      "0.65969706\n",
      "loss: [0.6510374]\n",
      "0.6580507\n",
      "loss: [0.6507801]\n",
      "0.65969706\n",
      "loss: [0.65048915]\n",
      "0.6603556\n",
      "loss: [0.6503722]\n",
      "0.66052026\n",
      "loss: [0.6504983]\n",
      "0.6621666\n",
      "loss: [0.6506373]\n",
      "0.6598617\n",
      "loss: [0.6506194]\n",
      "0.66117877\n",
      "loss: [0.6505454]\n",
      "0.6600264\n",
      "loss: [0.6503793]\n",
      "0.6623312\n",
      "loss: [0.65032554]\n",
      "0.6606849\n",
      "loss: [0.65030676]\n",
      "0.6608495\n",
      "loss: [0.6503416]\n",
      "0.6629898\n",
      "loss: [0.6503445]\n",
      "0.6606849\n",
      "loss: [0.65033454]\n",
      "0.6629898\n",
      "loss: [0.65036076]\n",
      "0.6603556\n",
      "loss: [0.650407]\n",
      "0.6616727\n",
      "loss: [0.65042067]\n",
      "0.6603556\n",
      "loss: [0.6504885]\n",
      "0.66183734\n",
      "loss: [0.6504822]\n",
      "0.6600264\n",
      "loss: [0.65056574]\n",
      "0.6615081\n",
      "loss: [0.6505421]\n",
      "0.6603556\n",
      "loss: [0.6504664]\n",
      "0.6616727\n",
      "loss: [0.6504308]\n",
      "0.6600264\n",
      "loss: [0.6503959]\n",
      "0.6624959\n",
      "loss: [0.65039307]\n",
      "0.66134346\n",
      "loss: [0.65034735]\n",
      "0.66282517\n",
      "loss: [0.6503368]\n",
      "0.66117877\n",
      "loss: [0.65035635]\n",
      "0.66200197\n",
      "loss: [0.6503607]\n",
      "0.6603556\n",
      "loss: [0.65036505]\n",
      "0.6621666\n",
      "loss: [0.6504087]\n",
      "0.6603556\n",
      "loss: [0.65058124]\n",
      "0.66052026\n",
      "loss: [0.65088856]\n",
      "0.6592032\n",
      "loss: [0.65121055]\n",
      "0.65821534\n",
      "loss: [0.6515481]\n",
      "0.65459335\n",
      "loss: [0.6515374]\n",
      "0.656569\n",
      "loss: [0.65103287]\n",
      "0.65821534\n",
      "loss: [0.650451]\n",
      "0.6615081\n",
      "loss: [0.6502659]\n",
      "0.660191\n",
      "loss: [0.6504865]\n",
      "0.6598617\n",
      "loss: [0.65081257]\n",
      "0.65969706\n",
      "loss: [0.6512182]\n",
      "0.65689826\n",
      "loss: [0.65121967]\n",
      "0.65903854\n",
      "loss: [0.65100807]\n",
      "0.65821534\n",
      "loss: [0.6505921]\n",
      "0.6615081\n",
      "loss: [0.6503462]\n",
      "0.66052026\n",
      "loss: [0.65046126]\n",
      "0.6603556\n",
      "loss: [0.6507763]\n",
      "0.6600264\n",
      "loss: [0.6510868]\n",
      "0.6578861\n",
      "loss: [0.6507985]\n",
      "0.6593678\n",
      "loss: [0.65059704]\n",
      "0.6598617\n",
      "loss: [0.6503941]\n",
      "0.66282517\n",
      "loss: [0.65037394]\n",
      "0.6621666\n",
      "loss: [0.6505478]\n",
      "0.6598617\n",
      "loss: [0.65094054]\n",
      "0.6595324\n",
      "loss: [0.6516237]\n",
      "0.6544287\n",
      "loss: [0.65192866]\n",
      "0.65591043\n",
      "loss: [0.6510068]\n",
      "0.6580507\n",
      "loss: [0.65036917]\n",
      "0.6615081\n",
      "loss: [0.6507948]\n",
      "0.660191\n",
      "loss: [0.65176266]\n",
      "0.65409946\n",
      "loss: [0.65207505]\n",
      "0.6552519\n",
      "loss: [0.65100276]\n",
      "0.6580507\n",
      "loss: [0.6503847]\n",
      "0.6603556\n",
      "loss: [0.65091366]\n",
      "0.65969706\n",
      "loss: [0.651506]\n",
      "0.65541655\n",
      "loss: [0.65107]\n",
      "0.6592032\n",
      "loss: [0.65045184]\n",
      "0.6608495\n",
      "loss: [0.6505241]\n",
      "0.66117877\n",
      "loss: [0.6507453]\n",
      "0.66117877\n",
      "loss: [0.6505874]\n",
      "0.6598617\n",
      "loss: [0.6504222]\n",
      "0.6629898\n",
      "loss: [0.6503808]\n",
      "0.66117877\n",
      "loss: [0.6503865]\n",
      "0.6606849\n",
      "loss: [0.65048146]\n",
      "0.6623312\n",
      "loss: [0.65042585]\n",
      "0.6603556\n",
      "loss: [0.6503113]\n",
      "0.6608495\n",
      "loss: [0.6503482]\n",
      "0.66183734\n",
      "loss: [0.650407]\n",
      "0.66117877\n",
      "loss: [0.6504582]\n",
      "0.6615081\n",
      "loss: [0.6509664]\n",
      "0.6570629\n",
      "loss: [0.6529172]\n",
      "0.65228844\n",
      "loss: [0.6540264]\n",
      "0.6499835\n",
      "loss: [0.65214235]\n",
      "0.6549226\n",
      "loss: [0.6503541]\n",
      "0.66117877\n",
      "loss: [0.65158904]\n",
      "0.6549226\n",
      "loss: [0.65318555]\n",
      "0.6519592\n",
      "loss: [0.651038]\n",
      "0.6575568\n",
      "loss: [0.65062606]\n",
      "0.66052026\n",
      "loss: [0.6516884]\n",
      "0.6564044\n",
      "loss: [0.65132344]\n",
      "0.65607506\n",
      "loss: [0.6505146]\n",
      "0.6621666\n",
      "loss: [0.6507134]\n",
      "0.6615081\n",
      "loss: [0.6513529]\n",
      "0.65739214\n",
      "loss: [0.6511625]\n",
      "0.6588739\n",
      "loss: [0.6505792]\n",
      "0.66101414\n",
      "loss: [0.6505957]\n",
      "0.6598617\n",
      "loss: [0.6510443]\n",
      "0.6593678\n",
      "loss: [0.65091026]\n",
      "0.6578861\n",
      "loss: [0.650469]\n",
      "0.66200197\n",
      "loss: [0.6505358]\n",
      "0.66266054\n",
      "loss: [0.650797]\n",
      "0.6595324\n",
      "loss: [0.650606]\n",
      "0.66282517\n",
      "loss: [0.65040165]\n",
      "0.6615081\n",
      "loss: [0.6505798]\n",
      "0.6598617\n",
      "loss: [0.6506788]\n",
      "0.6606849\n",
      "loss: [0.6505599]\n",
      "0.6595324\n",
      "loss: [0.65041083]\n",
      "0.6629898\n",
      "loss: [0.6503736]\n",
      "0.6621666\n",
      "loss: [0.6504261]\n",
      "0.660191\n",
      "loss: [0.6506049]\n",
      "0.6615081\n",
      "loss: [0.65067995]\n",
      "0.65903854\n",
      "loss: [0.6506994]\n",
      "0.6603556\n",
      "loss: [0.65072685]\n",
      "0.6593678\n",
      "loss: [0.6504831]\n",
      "0.66183734\n",
      "loss: [0.6503457]\n",
      "0.66134346\n",
      "loss: [0.6504698]\n",
      "0.660191\n",
      "loss: [0.6506918]\n",
      "0.6600264\n",
      "loss: [0.6506502]\n",
      "0.660191\n",
      "loss: [0.650458]\n",
      "0.6623312\n",
      "loss: [0.6503596]\n",
      "0.66183734\n",
      "loss: [0.650322]\n",
      "0.6616727\n",
      "loss: [0.65026486]\n",
      "0.66134346\n",
      "loss: [0.6502927]\n",
      "0.6606849\n",
      "loss: [0.6502895]\n",
      "0.66101414\n",
      "loss: [0.65025526]\n",
      "0.66052026\n",
      "loss: [0.6502456]\n",
      "0.6608495\n",
      "loss: [0.6502485]\n",
      "0.66266054\n",
      "loss: [0.6502222]\n",
      "0.6608495\n",
      "loss: [0.65021634]\n",
      "0.6624959\n",
      "loss: [0.6501999]\n",
      "0.6615081\n",
      "loss: [0.6502095]\n",
      "0.6623312\n",
      "loss: [0.65021354]\n",
      "0.6623312\n",
      "loss: [0.6502208]\n",
      "0.66183734\n",
      "loss: [0.65025264]\n",
      "0.66183734\n",
      "loss: [0.6503143]\n",
      "0.6608495\n",
      "loss: [0.6504129]\n",
      "0.66134346\n",
      "loss: [0.6505718]\n",
      "0.6592032\n",
      "loss: [0.6507266]\n",
      "0.6600264\n",
      "loss: [0.65064937]\n",
      "0.6588739\n",
      "loss: [0.65042174]\n",
      "0.6615081\n",
      "loss: [0.65024453]\n",
      "0.6615081\n",
      "loss: [0.6502048]\n",
      "0.6608495\n",
      "loss: [0.6502639]\n",
      "0.6621666\n",
      "loss: [0.650469]\n",
      "0.6608495\n",
      "loss: [0.6505352]\n",
      "0.6616727\n",
      "loss: [0.650664]\n",
      "0.6598617\n",
      "loss: [0.6506481]\n",
      "0.66052026\n",
      "loss: [0.6505726]\n",
      "0.6598617\n",
      "loss: [0.6504247]\n",
      "0.6616727\n",
      "loss: [0.65032774]\n",
      "0.6603556\n",
      "loss: [0.65020835]\n",
      "0.6616727\n",
      "loss: [0.6502167]\n",
      "0.66200197\n",
      "loss: [0.6503571]\n",
      "0.6606849\n",
      "loss: [0.6504168]\n",
      "0.66183734\n",
      "loss: [0.65052485]\n",
      "0.6598617\n",
      "loss: [0.6506127]\n",
      "0.6608495\n",
      "loss: [0.6505291]\n",
      "0.6598617\n",
      "loss: [0.65041554]\n",
      "0.66134346\n",
      "loss: [0.65027124]\n",
      "0.6606849\n",
      "loss: [0.6502159]\n",
      "0.6623312\n",
      "loss: [0.6502026]\n",
      "0.6616727\n",
      "loss: [0.6502363]\n",
      "0.6615081\n",
      "loss: [0.6503029]\n",
      "0.6629898\n",
      "loss: [0.65028566]\n",
      "0.6603556\n",
      "loss: [0.65033054]\n",
      "0.66282517\n",
      "loss: [0.65033436]\n",
      "0.66052026\n",
      "loss: [0.650358]\n",
      "0.66183734\n",
      "loss: [0.6503931]\n",
      "0.6603556\n",
      "loss: [0.65053177]\n",
      "0.66101414\n",
      "loss: [0.6506698]\n",
      "0.65903854\n",
      "loss: [0.65071833]\n",
      "0.6593678\n",
      "loss: [0.65073496]\n",
      "0.65903854\n",
      "loss: [0.6506738]\n",
      "0.6595324\n",
      "loss: [0.6504764]\n",
      "0.6600264\n",
      "loss: [0.65026987]\n",
      "0.6624959\n",
      "loss: [0.65028024]\n",
      "0.6624959\n",
      "loss: [0.65047765]\n",
      "0.66101414\n",
      "loss: [0.65063643]\n",
      "0.6608495\n",
      "loss: [0.65061593]\n",
      "0.6600264\n",
      "loss: [0.65065444]\n",
      "0.6606849\n",
      "loss: [0.65052813]\n",
      "0.660191\n",
      "loss: [0.6503513]\n",
      "0.6621666\n",
      "loss: [0.6502385]\n",
      "0.6606849\n",
      "loss: [0.65040123]\n",
      "0.6603556\n",
      "loss: [0.65087634]\n",
      "0.6598617\n",
      "loss: [0.6514176]\n",
      "0.65591043\n",
      "loss: [0.65160406]\n",
      "0.65673363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.65102184]\n",
      "0.6580507\n",
      "loss: [0.65047204]\n",
      "0.6608495\n",
      "loss: [0.6502432]\n",
      "0.6606849\n",
      "loss: [0.6504995]\n",
      "0.6603556\n",
      "loss: [0.65090424]\n",
      "0.6593678\n",
      "loss: [0.651097]\n",
      "0.6575568\n",
      "loss: [0.6510139]\n",
      "0.6595324\n",
      "loss: [0.65048313]\n",
      "0.660191\n",
      "loss: [0.6503395]\n",
      "0.6600264\n",
      "loss: [0.65096265]\n",
      "0.65969706\n",
      "loss: [0.6518175]\n",
      "0.6539348\n",
      "loss: [0.6514045]\n",
      "0.6572275\n",
      "loss: [0.65044254]\n",
      "0.6608495\n",
      "loss: [0.65054345]\n",
      "0.660191\n",
      "loss: [0.65119535]\n",
      "0.6598617\n",
      "loss: [0.65110546]\n",
      "0.6580507\n",
      "loss: [0.65050256]\n",
      "0.6623312\n",
      "loss: [0.65041214]\n",
      "0.6603556\n",
      "loss: [0.6505811]\n",
      "0.6600264\n",
      "loss: [0.6505413]\n",
      "0.66117877\n",
      "loss: [0.6504098]\n",
      "0.66052026\n",
      "loss: [0.65044075]\n",
      "0.660191\n",
      "loss: [0.6503974]\n",
      "0.6621666\n",
      "loss: [0.6504349]\n",
      "0.6603556\n",
      "loss: [0.65050316]\n",
      "0.6621666\n",
      "loss: [0.65041465]\n",
      "0.66052026\n",
      "loss: [0.65035695]\n",
      "0.6629898\n",
      "loss: [0.6504015]\n",
      "0.66117877\n",
      "loss: [0.6503398]\n",
      "0.66117877\n",
      "loss: [0.6505125]\n",
      "0.66183734\n",
      "loss: [0.6508468]\n",
      "0.6595324\n"
     ]
    }
   ],
   "source": [
    "n = g.number_of_nodes()\n",
    "n_hidden = 32\n",
    "n_embedding_updates = 1\n",
    "n_parameter_updates = 1\n",
    "alpha = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-1\n",
    "\n",
    "g.ndata['x'] = mx.nd.array([conv(i) for i in range(n)])\n",
    "#g.ndata['x'] = mx.nd.arange(n).reshape(n, 1)\n",
    "g.ndata['h'] = mx.nd.random.normal(shape=(n, n_hidden))\n",
    "\n",
    "steady_state_operator = SteadyStateOperator(n_hidden, 'relu')\n",
    "steady_state_operator.initialize()\n",
    "predictor = Predictor(n_hidden, 'relu')\n",
    "predictor.initialize()\n",
    "trainer = gluon.Trainer(predictor.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "def update_parameters(g, trainer):\n",
    "    with mx.autograd.record():\n",
    "        steady_state_operator(g)\n",
    "        z = predictor(g)\n",
    "        loss = loss_fcn(z, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "        print(\"loss: \" + str(loss.asnumpy()))\n",
    "    loss.backward()\n",
    "    trainer.step(1)  # divide gradients by the number of labelled nodes\n",
    "    return loss.asnumpy()[0]\n",
    "\n",
    "def train(g, trainer):\n",
    "     # first phase\n",
    "    for i in range(n_embedding_updates):\n",
    "        update_embeddings(g, steady_state_operator)\n",
    "    # second phase\n",
    "    for i in range(n_parameter_updates):\n",
    "        loss = update_parameters(g, trainer)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train(g, trainer)\n",
    "    acc = evaluate(g.ndata['z'], labels, eval_mask)\n",
    "    print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
