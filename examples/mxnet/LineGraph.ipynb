{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'mxnet'\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import math\n",
    "import numpy as np\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoint_chains(n_chains, length):\n",
    "    path_graph = nx.path_graph(n_chains * length).to_directed()\n",
    "    for i in range(n_chains - 1):  # break the path graph into N chains\n",
    "        path_graph.remove_edge((i + 1) * length - 1, (i + 1) * length)\n",
    "        path_graph.remove_edge((i + 1) * length, (i + 1) * length - 1)\n",
    "    for n in path_graph.nodes:\n",
    "        path_graph.add_edge(n, n)  # add self connections\n",
    "    return path_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = disjoint_chains(1, 30)\n",
    "nx.draw(g1, pos=nx.circular_layout(g1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g2 = disjoint_chains(2, 15)\n",
    "nx.draw(g2, pos=nx.circular_layout(g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 500\n",
    "n_epochs = 100\n",
    "lr = 3e-2\n",
    "weight_decay = 5e-4\n",
    "\n",
    "g = dgl.DGLGraph(disjoint_chains(1, 1000), readonly=True)\n",
    "labels = mx.nd.zeros([g.number_of_nodes()])\n",
    "labels[mx.nd.arange(0, g.number_of_nodes(), 2)] = 1\n",
    "#pattern = [0, 1]\n",
    "#for i in range(int(g.number_of_nodes()/len(pattern))):\n",
    "#    labels[i*len(pattern):(i + 1) * len(pattern)] = pattern\n",
    "\n",
    "train_mask = mx.nd.zeros((g.number_of_nodes()))\n",
    "train_mask[0:n_train_samples] = 1\n",
    "eval_mask = mx.nd.zeros((g.number_of_nodes()))\n",
    "eval_mask[n_train_samples:] = 1\n",
    "\n",
    "\n",
    "def evaluate(pred, labels, mask):\n",
    "    pred = pred.argmax(axis=1)\n",
    "    accuracy = ((pred == labels) * mask).sum() / mask.sum().asscalar()\n",
    "    return accuracy.asscalar()\n",
    "\n",
    "# Helper function to convert a number \n",
    "# to its fixed width binary representation\n",
    "def conv(x):\n",
    "  a = format(x, '032b')\n",
    "  l = list(str(a))\n",
    "  l = np.array(list(map(int, l)))\n",
    "  return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we predict with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.38786325], acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 16\n",
    "n_classes = 2\n",
    "\n",
    "class MLP(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "        self.dense2 = gluon.nn.Dense(n_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        return self.dense2(self.dense1(h))\n",
    "    \n",
    "model = MLP(n_hidden, n_classes, 'relu')\n",
    "model.initialize()\n",
    "features = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(10):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "print(\"loss: \" + str(loss.asnumpy()) + \", acc: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we define a GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 activation):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.dense = gluon.nn.Dense(out_feats, activation=activation)\n",
    "\n",
    "    def forward(self, h):\n",
    "        self.g.ndata['h'] = h\n",
    "        def concat_msg(edges):\n",
    "            return {'m': edges.src['h']}\n",
    "        def red_func(nodes):\n",
    "            m = nodes.mailbox['m']\n",
    "            if m.shape[1] == 3:\n",
    "                h = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "            else:\n",
    "                num_feats = m.shape[2]\n",
    "                m = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(m, mx.nd.zeros(shape=(m.shape[0], num_feats)), dim=1)\n",
    "            return {'h': self.dense(h)}\n",
    "        self.g.update_all(concat_msg, red_func)\n",
    "        return self.g.ndata.pop('h')\n",
    "\n",
    "class GCN(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = gluon.nn.Sequential()\n",
    "        # input layer\n",
    "        self.layers.add(GCNLayer(g, in_feats, n_hidden, activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.add(GCNLayer(g, n_hidden, n_hidden, activation))\n",
    "        self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "        self.dense2 = gluon.nn.Dense(n_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        return self.dense2(self.dense1(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dzzhen/Workspace/dgl/python/dgl/base.py:17: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.97\n",
      "0.992\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.924\n",
      "1.0\n",
      "0.974\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "degs = g.in_degrees().astype('float32')\n",
    "norm = mx.nd.power(degs, -0.5)\n",
    "g.ndata['norm'] = mx.nd.expand_dims(norm, 1)\n",
    "features = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "model = GCN(g, in_feats=features.shape[1], n_hidden=16, n_classes=2, n_layers=3, activation='relu', dropout=0.5)\n",
    "model.initialize()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "            {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "h = model(features)\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(n_epochs):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here we define an SSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteadyStateOperator(gluon.Block):\n",
    "    def __init__(self, n_hidden, activation, **kwargs):\n",
    "        super(SteadyStateOperator, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "            self.dense2 = gluon.nn.Dense(n_hidden)\n",
    "\n",
    "    def forward(self, g):\n",
    "        def apply_func(nodes):\n",
    "            z = mx.nd.concat(nodes.data['x'], nodes.data['h'], dim=1)\n",
    "            return {'h' : self.dense2(self.dense1(z))}\n",
    "\n",
    "        g.ndata['xh'] = mx.nd.concat(g.ndata['x'], g.ndata['h'], dim=1)\n",
    "        g.update_all(fn.copy_src(src='xh', out='m'), fn.sum(msg='m', out='h'))\n",
    "        g.apply_nodes(apply_func)\n",
    "        return g.ndata['h']\n",
    "\n",
    "def update_embeddings(g, steady_state_operator):\n",
    "    prev_h = g.ndata['h']\n",
    "    next_h = steady_state_operator(g)\n",
    "    g.ndata['h'] = (1 - alpha) * prev_h + alpha * next_h\n",
    "    \n",
    "class Predictor(gluon.Block):\n",
    "    def __init__(self, n_hidden, activation, **kwargs):\n",
    "        super(Predictor, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "            self.dense2 = gluon.nn.Dense(2)  ## binary classifier\n",
    "\n",
    "    def forward(self, g):\n",
    "        g.ndata['z'] = self.dense2(self.dense1(g.ndata['h']))\n",
    "        return g.ndata['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.6931463]\n",
      "0.502\n",
      "loss: [0.69318914]\n",
      "0.5\n",
      "loss: [0.69314003]\n",
      "0.5\n",
      "loss: [0.69316196]\n",
      "0.5\n",
      "loss: [0.6931619]\n",
      "0.5\n",
      "loss: [0.6931418]\n",
      "0.5\n",
      "loss: [0.6931329]\n",
      "0.5\n",
      "loss: [0.6931399]\n",
      "0.5\n",
      "loss: [0.6931447]\n",
      "0.5\n",
      "loss: [0.6931384]\n",
      "0.5\n",
      "loss: [0.6931291]\n",
      "0.5\n",
      "loss: [0.69312483]\n",
      "0.502\n",
      "loss: [0.693126]\n",
      "0.5\n",
      "loss: [0.6931263]\n",
      "0.5\n",
      "loss: [0.6931217]\n",
      "0.5\n",
      "loss: [0.6931142]\n",
      "0.5\n",
      "loss: [0.6931089]\n",
      "0.506\n",
      "loss: [0.69310665]\n",
      "0.5\n",
      "loss: [0.69310296]\n",
      "0.5\n",
      "loss: [0.6930952]\n",
      "0.5\n",
      "loss: [0.6930856]\n",
      "0.5\n",
      "loss: [0.69307727]\n",
      "0.502\n",
      "loss: [0.6930696]\n",
      "0.502\n",
      "loss: [0.693059]\n",
      "0.502\n",
      "loss: [0.6930455]\n",
      "0.502\n",
      "loss: [0.69303125]\n",
      "0.51\n",
      "loss: [0.693018]\n",
      "0.506\n",
      "loss: [0.6930007]\n",
      "0.502\n",
      "loss: [0.6929805]\n",
      "0.492\n",
      "loss: [0.6929591]\n",
      "0.52\n",
      "loss: [0.69293666]\n",
      "0.484\n",
      "loss: [0.6929107]\n",
      "0.51\n",
      "loss: [0.69288164]\n",
      "0.506\n",
      "loss: [0.69285005]\n",
      "0.516\n",
      "loss: [0.6928142]\n",
      "0.518\n",
      "loss: [0.6927751]\n",
      "0.508\n",
      "loss: [0.692731]\n",
      "0.506\n",
      "loss: [0.69268185]\n",
      "0.506\n",
      "loss: [0.6926273]\n",
      "0.51\n",
      "loss: [0.69256693]\n",
      "0.508\n",
      "loss: [0.6925006]\n",
      "0.502\n",
      "loss: [0.6924272]\n",
      "0.508\n",
      "loss: [0.69234824]\n",
      "0.508\n",
      "loss: [0.69226223]\n",
      "0.514\n",
      "loss: [0.69216824]\n",
      "0.518\n",
      "loss: [0.6920673]\n",
      "0.502\n",
      "loss: [0.69196]\n",
      "0.51\n",
      "loss: [0.69184595]\n",
      "0.518\n",
      "loss: [0.69172823]\n",
      "0.52\n",
      "loss: [0.6916078]\n",
      "0.518\n",
      "loss: [0.6914674]\n",
      "0.524\n",
      "loss: [0.6913283]\n",
      "0.516\n",
      "loss: [0.6912001]\n",
      "0.524\n",
      "loss: [0.69104266]\n",
      "0.522\n",
      "loss: [0.69088966]\n",
      "0.512\n",
      "loss: [0.6907486]\n",
      "0.52\n",
      "loss: [0.69058144]\n",
      "0.512\n",
      "loss: [0.690427]\n",
      "0.516\n",
      "loss: [0.69028217]\n",
      "0.516\n",
      "loss: [0.6901183]\n",
      "0.512\n",
      "loss: [0.6899634]\n",
      "0.506\n",
      "loss: [0.6898233]\n",
      "0.514\n",
      "loss: [0.68967795]\n",
      "0.516\n",
      "loss: [0.6895318]\n",
      "0.502\n",
      "loss: [0.68939424]\n",
      "0.504\n",
      "loss: [0.6892665]\n",
      "0.514\n",
      "loss: [0.6891481]\n",
      "0.514\n",
      "loss: [0.6890305]\n",
      "0.516\n",
      "loss: [0.68891066]\n",
      "0.516\n",
      "loss: [0.6887915]\n",
      "0.5\n",
      "loss: [0.68869245]\n",
      "0.502\n",
      "loss: [0.6886107]\n",
      "0.516\n",
      "loss: [0.6885294]\n",
      "0.51\n",
      "loss: [0.6884463]\n",
      "0.516\n",
      "loss: [0.6883581]\n",
      "0.502\n",
      "loss: [0.6882923]\n",
      "0.504\n",
      "loss: [0.68824565]\n",
      "0.516\n",
      "loss: [0.6881933]\n",
      "0.508\n",
      "loss: [0.6881353]\n",
      "0.51\n",
      "loss: [0.68808174]\n",
      "0.508\n",
      "loss: [0.68804777]\n",
      "0.506\n",
      "loss: [0.68802345]\n",
      "0.512\n",
      "loss: [0.6879876]\n",
      "0.512\n",
      "loss: [0.6879524]\n",
      "0.51\n",
      "loss: [0.68792856]\n",
      "0.504\n",
      "loss: [0.6879154]\n",
      "0.51\n",
      "loss: [0.6879074]\n",
      "0.514\n",
      "loss: [0.687887]\n",
      "0.508\n",
      "loss: [0.6878717]\n",
      "0.506\n",
      "loss: [0.6878685]\n",
      "0.508\n",
      "loss: [0.687867]\n",
      "0.51\n",
      "loss: [0.6878622]\n",
      "0.508\n",
      "loss: [0.68785363]\n",
      "0.506\n",
      "loss: [0.6878543]\n",
      "0.504\n",
      "loss: [0.687861]\n",
      "0.508\n",
      "loss: [0.68786174]\n",
      "0.508\n",
      "loss: [0.6878629]\n",
      "0.506\n",
      "loss: [0.68786883]\n",
      "0.504\n",
      "loss: [0.6878782]\n",
      "0.51\n",
      "loss: [0.687887]\n",
      "0.512\n",
      "loss: [0.68789256]\n",
      "0.506\n",
      "loss: [0.6879023]\n",
      "0.506\n",
      "loss: [0.6879152]\n",
      "0.512\n",
      "loss: [0.68792486]\n",
      "0.5\n",
      "loss: [0.6879344]\n",
      "0.502\n",
      "loss: [0.68794525]\n",
      "0.502\n",
      "loss: [0.68795687]\n",
      "0.502\n",
      "loss: [0.6879678]\n",
      "0.504\n",
      "loss: [0.6879764]\n",
      "0.506\n",
      "loss: [0.6879866]\n",
      "0.506\n",
      "loss: [0.6879994]\n",
      "0.508\n",
      "loss: [0.6880086]\n",
      "0.502\n",
      "loss: [0.68801695]\n",
      "0.506\n",
      "loss: [0.68802583]\n",
      "0.504\n",
      "loss: [0.68803376]\n",
      "0.502\n",
      "loss: [0.6880405]\n",
      "0.504\n",
      "loss: [0.68804556]\n",
      "0.506\n",
      "loss: [0.6880525]\n",
      "0.508\n",
      "loss: [0.68805903]\n",
      "0.504\n",
      "loss: [0.6880641]\n",
      "0.506\n",
      "loss: [0.6880687]\n",
      "0.508\n",
      "loss: [0.6880727]\n",
      "0.502\n",
      "loss: [0.68807566]\n",
      "0.506\n",
      "loss: [0.6880782]\n",
      "0.504\n",
      "loss: [0.68808013]\n",
      "0.506\n",
      "loss: [0.68808156]\n",
      "0.508\n",
      "loss: [0.6880827]\n",
      "0.504\n",
      "loss: [0.6880831]\n",
      "0.508\n",
      "loss: [0.68808347]\n",
      "0.506\n",
      "loss: [0.6880854]\n",
      "0.502\n",
      "loss: [0.68808603]\n",
      "0.506\n",
      "loss: [0.6880863]\n",
      "0.504\n",
      "loss: [0.6880852]\n",
      "0.506\n",
      "loss: [0.68808395]\n",
      "0.506\n",
      "loss: [0.68808246]\n",
      "0.504\n",
      "loss: [0.6880809]\n",
      "0.508\n",
      "loss: [0.68807954]\n",
      "0.506\n",
      "loss: [0.6880781]\n",
      "0.504\n",
      "loss: [0.68807536]\n",
      "0.506\n",
      "loss: [0.6880728]\n",
      "0.502\n",
      "loss: [0.68807]\n",
      "0.506\n",
      "loss: [0.6880675]\n",
      "0.506\n",
      "loss: [0.68806547]\n",
      "0.502\n",
      "loss: [0.68806285]\n",
      "0.506\n",
      "loss: [0.688061]\n",
      "0.504\n",
      "loss: [0.68805903]\n",
      "0.506\n",
      "loss: [0.688059]\n",
      "0.506\n",
      "loss: [0.6880587]\n",
      "0.508\n",
      "loss: [0.6880583]\n",
      "0.506\n",
      "loss: [0.6880594]\n",
      "0.504\n",
      "loss: [0.6880596]\n",
      "0.506\n",
      "loss: [0.6880596]\n",
      "0.504\n",
      "loss: [0.6880578]\n",
      "0.506\n",
      "loss: [0.688056]\n",
      "0.502\n",
      "loss: [0.6880534]\n",
      "0.508\n",
      "loss: [0.68805116]\n",
      "0.508\n",
      "loss: [0.6880493]\n",
      "0.504\n",
      "loss: [0.6880474]\n",
      "0.506\n",
      "loss: [0.6880465]\n",
      "0.504\n",
      "loss: [0.6880453]\n",
      "0.506\n",
      "loss: [0.6880473]\n",
      "0.504\n",
      "loss: [0.6880483]\n",
      "0.504\n",
      "loss: [0.68804985]\n",
      "0.504\n",
      "loss: [0.68804973]\n",
      "0.506\n",
      "loss: [0.68805]\n",
      "0.504\n",
      "loss: [0.68804926]\n",
      "0.506\n",
      "loss: [0.6880505]\n",
      "0.504\n",
      "loss: [0.6880508]\n",
      "0.506\n",
      "loss: [0.6880512]\n",
      "0.504\n",
      "loss: [0.6880504]\n",
      "0.506\n",
      "loss: [0.68805015]\n",
      "0.504\n",
      "loss: [0.688049]\n",
      "0.506\n",
      "loss: [0.6880487]\n",
      "0.502\n",
      "loss: [0.6880479]\n",
      "0.506\n",
      "loss: [0.6880479]\n",
      "0.502\n",
      "loss: [0.68804777]\n",
      "0.508\n",
      "loss: [0.68804824]\n",
      "0.504\n",
      "loss: [0.6880487]\n",
      "0.508\n",
      "loss: [0.688051]\n",
      "0.504\n",
      "loss: [0.6880527]\n",
      "0.506\n",
      "loss: [0.68805474]\n",
      "0.504\n",
      "loss: [0.688056]\n",
      "0.504\n",
      "loss: [0.6880607]\n",
      "0.51\n",
      "loss: [0.68806446]\n",
      "0.512\n",
      "loss: [0.6880714]\n",
      "0.514\n",
      "loss: [0.68807405]\n",
      "0.51\n",
      "loss: [0.6880774]\n",
      "0.514\n",
      "loss: [0.6880701]\n",
      "0.51\n",
      "loss: [0.688063]\n",
      "0.51\n",
      "loss: [0.68805695]\n",
      "0.508\n",
      "loss: [0.6880584]\n",
      "0.502\n",
      "loss: [0.68806463]\n",
      "0.506\n",
      "loss: [0.6880662]\n",
      "0.51\n",
      "loss: [0.6880679]\n",
      "0.508\n",
      "loss: [0.68806386]\n",
      "0.502\n",
      "loss: [0.6880635]\n",
      "0.508\n",
      "loss: [0.6880671]\n",
      "0.51\n",
      "loss: [0.68806946]\n",
      "0.506\n",
      "loss: [0.68807364]\n",
      "0.508\n",
      "loss: [0.6880699]\n",
      "0.506\n",
      "loss: [0.688067]\n",
      "0.504\n",
      "loss: [0.68806654]\n",
      "0.504\n",
      "loss: [0.688067]\n",
      "0.506\n",
      "loss: [0.6880674]\n",
      "0.508\n",
      "loss: [0.68806285]\n",
      "0.502\n",
      "loss: [0.6880599]\n",
      "0.502\n",
      "loss: [0.68805903]\n",
      "0.504\n",
      "loss: [0.68805903]\n",
      "0.502\n",
      "loss: [0.6880597]\n",
      "0.512\n",
      "loss: [0.68805736]\n",
      "0.502\n",
      "loss: [0.6880564]\n",
      "0.504\n",
      "loss: [0.6880564]\n",
      "0.504\n",
      "loss: [0.6880571]\n",
      "0.504\n",
      "loss: [0.6880602]\n",
      "0.512\n",
      "loss: [0.6880602]\n",
      "0.502\n",
      "loss: [0.68806297]\n",
      "0.51\n",
      "loss: [0.6880626]\n",
      "0.506\n",
      "loss: [0.68806255]\n",
      "0.506\n",
      "loss: [0.6880625]\n",
      "0.504\n",
      "loss: [0.68806136]\n",
      "0.504\n",
      "loss: [0.6880601]\n",
      "0.506\n",
      "loss: [0.68805677]\n",
      "0.506\n",
      "loss: [0.6880543]\n",
      "0.504\n",
      "loss: [0.68805224]\n",
      "0.504\n",
      "loss: [0.6880509]\n",
      "0.506\n",
      "loss: [0.6880507]\n",
      "0.504\n",
      "loss: [0.6880499]\n",
      "0.506\n",
      "loss: [0.6880519]\n",
      "0.504\n",
      "loss: [0.6880526]\n",
      "0.506\n",
      "loss: [0.6880537]\n",
      "0.506\n",
      "loss: [0.6880539]\n",
      "0.508\n",
      "loss: [0.6880541]\n",
      "0.506\n",
      "loss: [0.6880542]\n",
      "0.504\n",
      "loss: [0.68805355]\n",
      "0.506\n",
      "loss: [0.68805516]\n",
      "0.506\n",
      "loss: [0.68805504]\n",
      "0.502\n",
      "loss: [0.6880552]\n",
      "0.506\n",
      "loss: [0.68805355]\n",
      "0.506\n",
      "loss: [0.6880526]\n",
      "0.504\n",
      "loss: [0.6880517]\n",
      "0.504\n",
      "loss: [0.6880511]\n",
      "0.506\n",
      "loss: [0.68805134]\n",
      "0.504\n",
      "loss: [0.6880507]\n",
      "0.506\n",
      "loss: [0.68805104]\n",
      "0.504\n",
      "loss: [0.6880503]\n",
      "0.506\n",
      "loss: [0.6880506]\n",
      "0.502\n",
      "loss: [0.6880507]\n",
      "0.508\n",
      "loss: [0.6880524]\n",
      "0.504\n",
      "loss: [0.6880538]\n",
      "0.508\n",
      "loss: [0.68805474]\n",
      "0.504\n",
      "loss: [0.6880553]\n",
      "0.508\n",
      "loss: [0.6880573]\n",
      "0.504\n",
      "loss: [0.6880584]\n",
      "0.506\n",
      "loss: [0.6880595]\n",
      "0.504\n",
      "loss: [0.6880586]\n",
      "0.502\n",
      "loss: [0.68805826]\n",
      "0.506\n",
      "loss: [0.6880562]\n",
      "0.502\n",
      "loss: [0.68805504]\n",
      "0.504\n",
      "loss: [0.688053]\n",
      "0.506\n",
      "loss: [0.6880522]\n",
      "0.502\n",
      "loss: [0.68805134]\n",
      "0.508\n",
      "loss: [0.68805295]\n",
      "0.504\n",
      "loss: [0.68805414]\n",
      "0.506\n",
      "loss: [0.6880556]\n",
      "0.504\n",
      "loss: [0.6880558]\n",
      "0.506\n",
      "loss: [0.68805623]\n",
      "0.504\n",
      "loss: [0.6880554]\n",
      "0.506\n",
      "loss: [0.68805766]\n",
      "0.506\n",
      "loss: [0.6880584]\n",
      "0.504\n",
      "loss: [0.6880604]\n",
      "0.508\n",
      "loss: [0.68805987]\n",
      "0.506\n",
      "loss: [0.68806136]\n",
      "0.508\n",
      "loss: [0.68805957]\n",
      "0.514\n",
      "loss: [0.68806005]\n",
      "0.508\n",
      "loss: [0.6880575]\n",
      "0.506\n",
      "loss: [0.6880571]\n",
      "0.508\n",
      "loss: [0.68805444]\n",
      "0.504\n",
      "loss: [0.68805397]\n",
      "0.504\n",
      "loss: [0.6880534]\n",
      "0.508\n",
      "loss: [0.68805397]\n",
      "0.506\n",
      "loss: [0.68805546]\n",
      "0.504\n",
      "loss: [0.6880565]\n",
      "0.502\n",
      "loss: [0.6880605]\n",
      "0.51\n",
      "loss: [0.68806225]\n",
      "0.508\n",
      "loss: [0.6880691]\n",
      "0.508\n",
      "loss: [0.68807054]\n",
      "0.51\n",
      "loss: [0.68807197]\n",
      "0.506\n",
      "loss: [0.6880678]\n",
      "0.508\n",
      "loss: [0.68806505]\n",
      "0.506\n",
      "loss: [0.68806183]\n",
      "0.508\n",
      "loss: [0.6880603]\n",
      "0.506\n",
      "loss: [0.68806064]\n",
      "0.51\n",
      "loss: [0.68805957]\n",
      "0.506\n",
      "loss: [0.6880599]\n",
      "0.508\n",
      "loss: [0.68805754]\n",
      "0.506\n",
      "loss: [0.6880594]\n",
      "0.512\n",
      "loss: [0.68805885]\n",
      "0.502\n",
      "loss: [0.6880594]\n",
      "0.504\n",
      "loss: [0.6880593]\n",
      "0.508\n",
      "loss: [0.6880594]\n",
      "0.506\n",
      "loss: [0.68805987]\n",
      "0.504\n",
      "loss: [0.6880589]\n",
      "0.502\n",
      "loss: [0.6880617]\n",
      "0.51\n",
      "loss: [0.6880616]\n",
      "0.506\n",
      "loss: [0.688062]\n",
      "0.51\n",
      "loss: [0.6880593]\n",
      "0.504\n",
      "loss: [0.68805766]\n",
      "0.504\n",
      "loss: [0.6880553]\n",
      "0.506\n",
      "loss: [0.6880541]\n",
      "0.508\n",
      "loss: [0.6880539]\n",
      "0.504\n",
      "loss: [0.68805325]\n",
      "0.504\n",
      "loss: [0.68805385]\n",
      "0.508\n",
      "loss: [0.68805283]\n",
      "0.502\n",
      "loss: [0.6880531]\n",
      "0.508\n",
      "loss: [0.68805206]\n",
      "0.506\n",
      "loss: [0.688054]\n",
      "0.504\n",
      "loss: [0.68805456]\n",
      "0.506\n",
      "loss: [0.68805546]\n",
      "0.504\n",
      "loss: [0.6880553]\n",
      "0.508\n",
      "loss: [0.68805504]\n",
      "0.506\n",
      "loss: [0.6880546]\n",
      "0.504\n",
      "loss: [0.6880555]\n",
      "0.506\n",
      "loss: [0.6880561]\n",
      "0.506\n",
      "loss: [0.68805665]\n",
      "0.502\n",
      "loss: [0.6880561]\n",
      "0.506\n",
      "loss: [0.68805593]\n",
      "0.504\n",
      "loss: [0.6880546]\n",
      "0.506\n",
      "loss: [0.6880542]\n",
      "0.504\n",
      "loss: [0.6880528]\n",
      "0.506\n",
      "loss: [0.6880525]\n",
      "0.504\n",
      "loss: [0.68805164]\n",
      "0.506\n",
      "loss: [0.6880538]\n",
      "0.504\n",
      "loss: [0.6880548]\n",
      "0.502\n",
      "loss: [0.6880567]\n",
      "0.508\n",
      "loss: [0.6880565]\n",
      "0.502\n",
      "loss: [0.68805736]\n",
      "0.512\n",
      "loss: [0.6880562]\n",
      "0.504\n",
      "loss: [0.6880596]\n",
      "0.508\n",
      "loss: [0.6880654]\n",
      "0.51\n",
      "loss: [0.6880735]\n",
      "0.514\n",
      "loss: [0.6880746]\n",
      "0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.68807584]\n",
      "0.516\n",
      "loss: [0.688073]\n",
      "0.512\n",
      "loss: [0.68807006]\n",
      "0.508\n",
      "loss: [0.6880639]\n",
      "0.502\n",
      "loss: [0.6880622]\n",
      "0.508\n",
      "loss: [0.6880641]\n",
      "0.51\n",
      "loss: [0.6880652]\n",
      "0.504\n",
      "loss: [0.68806773]\n",
      "0.508\n",
      "loss: [0.6880727]\n",
      "0.512\n",
      "loss: [0.6880766]\n",
      "0.51\n",
      "loss: [0.6880728]\n",
      "0.508\n",
      "loss: [0.68807155]\n",
      "0.502\n",
      "loss: [0.6880724]\n",
      "0.504\n",
      "loss: [0.6880741]\n",
      "0.506\n",
      "loss: [0.6880764]\n",
      "0.51\n",
      "loss: [0.68807226]\n",
      "0.508\n",
      "loss: [0.6880688]\n",
      "0.508\n",
      "loss: [0.6880647]\n",
      "0.508\n",
      "loss: [0.68806314]\n",
      "0.506\n",
      "loss: [0.6880638]\n",
      "0.512\n",
      "loss: [0.68806165]\n",
      "0.506\n",
      "loss: [0.6880604]\n",
      "0.51\n",
      "loss: [0.6880574]\n",
      "0.506\n",
      "loss: [0.68805677]\n",
      "0.508\n",
      "loss: [0.68805766]\n",
      "0.504\n",
      "loss: [0.68805796]\n",
      "0.502\n",
      "loss: [0.68805915]\n",
      "0.508\n",
      "loss: [0.6880576]\n",
      "0.504\n",
      "loss: [0.688057]\n",
      "0.502\n",
      "loss: [0.688056]\n",
      "0.506\n",
      "loss: [0.68805534]\n",
      "0.506\n",
      "loss: [0.68805736]\n",
      "0.508\n",
      "loss: [0.68805724]\n",
      "0.502\n",
      "loss: [0.68805754]\n",
      "0.508\n",
      "loss: [0.6880553]\n",
      "0.506\n",
      "loss: [0.6880541]\n",
      "0.502\n",
      "loss: [0.688053]\n",
      "0.504\n",
      "loss: [0.6880525]\n",
      "0.506\n",
      "loss: [0.6880556]\n",
      "0.512\n",
      "loss: [0.6880564]\n",
      "0.506\n",
      "loss: [0.68805766]\n",
      "0.51\n",
      "loss: [0.6880554]\n",
      "0.502\n",
      "loss: [0.6880545]\n",
      "0.502\n",
      "loss: [0.68805355]\n",
      "0.504\n",
      "loss: [0.6880532]\n",
      "0.506\n",
      "loss: [0.68805397]\n",
      "0.508\n",
      "loss: [0.68805283]\n",
      "0.502\n",
      "loss: [0.6880554]\n",
      "0.512\n",
      "loss: [0.688055]\n",
      "0.502\n",
      "loss: [0.68805534]\n",
      "0.504\n",
      "loss: [0.6880542]\n",
      "0.506\n",
      "loss: [0.6880538]\n",
      "0.508\n",
      "loss: [0.68805385]\n",
      "0.504\n",
      "loss: [0.6880531]\n",
      "0.504\n",
      "loss: [0.68805623]\n",
      "0.512\n",
      "loss: [0.6880566]\n",
      "0.506\n",
      "loss: [0.6880576]\n",
      "0.512\n",
      "loss: [0.6880555]\n",
      "0.502\n",
      "loss: [0.68805474]\n",
      "0.504\n",
      "loss: [0.6880538]\n",
      "0.506\n",
      "loss: [0.6880534]\n",
      "0.506\n",
      "loss: [0.68805414]\n",
      "0.506\n",
      "loss: [0.6880535]\n",
      "0.502\n",
      "loss: [0.688054]\n",
      "0.508\n",
      "loss: [0.6880528]\n",
      "0.506\n",
      "loss: [0.68805283]\n",
      "0.504\n",
      "loss: [0.68805236]\n",
      "0.508\n",
      "loss: [0.68805254]\n",
      "0.508\n",
      "loss: [0.6880551]\n",
      "0.504\n",
      "loss: [0.6880565]\n",
      "0.502\n",
      "loss: [0.6880587]\n",
      "0.512\n",
      "loss: [0.6880577]\n",
      "0.502\n",
      "loss: [0.68805766]\n",
      "0.508\n",
      "loss: [0.6880556]\n",
      "0.506\n",
      "loss: [0.68805736]\n",
      "0.504\n",
      "loss: [0.6880574]\n",
      "0.506\n",
      "loss: [0.6880578]\n",
      "0.502\n",
      "loss: [0.6880571]\n",
      "0.508\n",
      "loss: [0.6880565]\n",
      "0.506\n",
      "loss: [0.6880558]\n",
      "0.504\n",
      "loss: [0.68805474]\n",
      "0.506\n",
      "loss: [0.68805426]\n",
      "0.502\n",
      "loss: [0.68805325]\n",
      "0.506\n",
      "loss: [0.68805516]\n",
      "0.504\n",
      "loss: [0.6880559]\n",
      "0.502\n",
      "loss: [0.68805724]\n",
      "0.508\n",
      "loss: [0.68805635]\n",
      "0.502\n",
      "loss: [0.6880563]\n",
      "0.506\n",
      "loss: [0.6880544]\n",
      "0.506\n",
      "loss: [0.68805337]\n",
      "0.502\n",
      "loss: [0.68805194]\n",
      "0.508\n",
      "loss: [0.6880529]\n",
      "0.502\n",
      "loss: [0.6880533]\n",
      "0.506\n",
      "loss: [0.6880541]\n",
      "0.504\n",
      "loss: [0.6880536]\n",
      "0.506\n",
      "loss: [0.6880537]\n",
      "0.504\n",
      "loss: [0.6880527]\n",
      "0.506\n",
      "loss: [0.6880525]\n",
      "0.502\n",
      "loss: [0.6880516]\n",
      "0.506\n",
      "loss: [0.68805134]\n",
      "0.504\n",
      "loss: [0.68805087]\n",
      "0.508\n",
      "loss: [0.68805075]\n",
      "0.504\n",
      "loss: [0.6880506]\n",
      "0.506\n",
      "loss: [0.6880507]\n",
      "0.506\n",
      "loss: [0.6880525]\n",
      "0.504\n",
      "loss: [0.68805367]\n",
      "0.506\n",
      "loss: [0.6880552]\n",
      "0.504\n",
      "loss: [0.6880563]\n",
      "0.502\n",
      "loss: [0.68805856]\n",
      "0.512\n",
      "loss: [0.6880613]\n",
      "0.506\n",
      "loss: [0.6880719]\n",
      "0.516\n",
      "loss: [0.6880856]\n",
      "0.51\n",
      "loss: [0.6881034]\n",
      "0.512\n",
      "loss: [0.6881159]\n",
      "0.526\n",
      "loss: [0.6881158]\n",
      "0.51\n",
      "loss: [0.6880906]\n",
      "0.51\n",
      "loss: [0.68807924]\n",
      "0.504\n",
      "loss: [0.6880886]\n",
      "0.51\n",
      "loss: [0.6880977]\n",
      "0.508\n",
      "loss: [0.68809605]\n",
      "0.512\n",
      "loss: [0.68807787]\n",
      "0.506\n",
      "loss: [0.6880739]\n",
      "0.506\n",
      "loss: [0.6880842]\n",
      "0.514\n",
      "loss: [0.6880854]\n",
      "0.51\n",
      "loss: [0.6880799]\n",
      "0.51\n",
      "loss: [0.6880737]\n",
      "0.506\n",
      "loss: [0.688078]\n",
      "0.504\n",
      "loss: [0.68808496]\n",
      "0.51\n",
      "loss: [0.6880794]\n",
      "0.504\n",
      "loss: [0.6880753]\n",
      "0.502\n",
      "loss: [0.688077]\n",
      "0.51\n",
      "loss: [0.6880782]\n",
      "0.51\n",
      "loss: [0.6880768]\n",
      "0.508\n",
      "loss: [0.68807024]\n",
      "0.506\n",
      "loss: [0.68806857]\n",
      "0.506\n",
      "loss: [0.6880701]\n",
      "0.51\n",
      "loss: [0.68806654]\n",
      "0.506\n",
      "loss: [0.68806547]\n",
      "0.506\n",
      "loss: [0.6880635]\n",
      "0.506\n",
      "loss: [0.68806356]\n",
      "0.504\n",
      "loss: [0.68806446]\n",
      "0.512\n",
      "loss: [0.6880612]\n",
      "0.502\n",
      "loss: [0.68805873]\n",
      "0.504\n",
      "loss: [0.6880572]\n",
      "0.504\n",
      "loss: [0.68805534]\n",
      "0.502\n",
      "loss: [0.68805385]\n",
      "0.506\n",
      "loss: [0.68805045]\n",
      "0.506\n",
      "loss: [0.6880487]\n",
      "0.508\n",
      "loss: [0.6880512]\n",
      "0.508\n",
      "loss: [0.6880515]\n",
      "0.502\n",
      "loss: [0.6880517]\n",
      "0.504\n",
      "loss: [0.68805015]\n",
      "0.506\n",
      "loss: [0.6880502]\n",
      "0.506\n",
      "loss: [0.68805146]\n",
      "0.506\n",
      "loss: [0.68805]\n",
      "0.504\n",
      "loss: [0.6880492]\n",
      "0.502\n",
      "loss: [0.6880483]\n",
      "0.504\n",
      "loss: [0.6880481]\n",
      "0.506\n",
      "loss: [0.6880487]\n",
      "0.504\n",
      "loss: [0.6880477]\n",
      "0.506\n",
      "loss: [0.68804914]\n",
      "0.502\n",
      "loss: [0.68804973]\n",
      "0.508\n",
      "loss: [0.68805045]\n",
      "0.506\n",
      "loss: [0.6880514]\n",
      "0.504\n",
      "loss: [0.688051]\n",
      "0.506\n",
      "loss: [0.68805087]\n",
      "0.504\n",
      "loss: [0.6880503]\n",
      "0.506\n",
      "loss: [0.68805003]\n",
      "0.508\n",
      "loss: [0.68805027]\n",
      "0.502\n",
      "loss: [0.68805015]\n",
      "0.506\n",
      "loss: [0.6880529]\n",
      "0.504\n",
      "loss: [0.68805456]\n",
      "0.506\n",
      "loss: [0.6880563]\n",
      "0.504\n",
      "loss: [0.6880572]\n",
      "0.506\n",
      "loss: [0.68805754]\n",
      "0.506\n",
      "loss: [0.6880578]\n",
      "0.504\n",
      "loss: [0.68805677]\n",
      "0.506\n",
      "loss: [0.6880559]\n",
      "0.504\n",
      "loss: [0.68805665]\n",
      "0.504\n",
      "loss: [0.6880568]\n",
      "0.506\n",
      "loss: [0.68805724]\n",
      "0.504\n",
      "loss: [0.68805563]\n",
      "0.506\n",
      "loss: [0.68805456]\n",
      "0.504\n",
      "loss: [0.6880535]\n",
      "0.504\n",
      "loss: [0.6880522]\n",
      "0.506\n",
      "loss: [0.6880516]\n",
      "0.504\n",
      "loss: [0.68805027]\n",
      "0.506\n",
      "loss: [0.6880518]\n",
      "0.504\n",
      "loss: [0.6880524]\n",
      "0.506\n",
      "loss: [0.68805337]\n",
      "0.504\n",
      "loss: [0.6880537]\n",
      "0.506\n",
      "loss: [0.6880538]\n",
      "0.506\n",
      "loss: [0.688054]\n",
      "0.504\n",
      "loss: [0.68805325]\n",
      "0.506\n",
      "loss: [0.68805283]\n",
      "0.504\n",
      "loss: [0.68805206]\n",
      "0.508\n",
      "loss: [0.6880515]\n",
      "0.508\n",
      "loss: [0.6880529]\n",
      "0.502\n",
      "loss: [0.68805337]\n",
      "0.506\n",
      "loss: [0.6880541]\n",
      "0.504\n",
      "loss: [0.68805325]\n",
      "0.506\n",
      "loss: [0.68805283]\n",
      "0.504\n",
      "loss: [0.68805224]\n",
      "0.504\n",
      "loss: [0.68805146]\n",
      "0.506\n",
      "loss: [0.68805134]\n",
      "0.502\n",
      "loss: [0.6880512]\n",
      "0.506\n",
      "loss: [0.6880549]\n",
      "0.506\n",
      "loss: [0.68805623]\n",
      "0.502\n",
      "loss: [0.6880577]\n",
      "0.504\n",
      "loss: [0.6880572]\n",
      "0.506\n",
      "loss: [0.68805707]\n",
      "0.508\n",
      "loss: [0.6880569]\n",
      "0.502\n",
      "loss: [0.68805575]\n",
      "0.506\n",
      "loss: [0.68805534]\n",
      "0.504\n",
      "loss: [0.68805426]\n",
      "0.506\n",
      "loss: [0.6880566]\n",
      "0.506\n",
      "loss: [0.6880587]\n",
      "0.502\n",
      "loss: [0.68806094]\n",
      "0.508\n",
      "loss: [0.68806]\n",
      "0.506\n",
      "loss: [0.6880593]\n",
      "0.504\n",
      "loss: [0.6880578]\n",
      "0.504\n",
      "loss: [0.688056]\n",
      "0.506\n",
      "loss: [0.6880551]\n",
      "0.504\n",
      "loss: [0.68805814]\n",
      "0.504\n",
      "loss: [0.68806344]\n",
      "0.51\n",
      "loss: [0.68806577]\n",
      "0.512\n",
      "loss: [0.6880686]\n",
      "0.51\n",
      "loss: [0.6880655]\n",
      "0.506\n",
      "loss: [0.6880643]\n",
      "0.502\n",
      "loss: [0.6880632]\n",
      "0.506\n",
      "loss: [0.6880629]\n",
      "0.504\n",
      "loss: [0.6880643]\n",
      "0.51\n",
      "loss: [0.6880619]\n",
      "0.506\n",
      "loss: [0.6880607]\n",
      "0.51\n",
      "loss: [0.6880571]\n",
      "0.506\n",
      "loss: [0.6880555]\n",
      "0.506\n",
      "loss: [0.6880549]\n",
      "0.504\n",
      "loss: [0.6880543]\n",
      "0.504\n",
      "loss: [0.68805534]\n",
      "0.508\n",
      "loss: [0.68805426]\n",
      "0.502\n",
      "loss: [0.6880548]\n",
      "0.504\n",
      "loss: [0.6880543]\n",
      "0.508\n",
      "loss: [0.6880548]\n",
      "0.508\n",
      "loss: [0.6880556]\n",
      "0.502\n",
      "loss: [0.68805563]\n",
      "0.506\n",
      "loss: [0.68805903]\n",
      "0.508\n",
      "loss: [0.68805957]\n",
      "0.502\n",
      "loss: [0.6880608]\n",
      "0.51\n",
      "loss: [0.6880588]\n",
      "0.504\n",
      "loss: [0.6880578]\n",
      "0.504\n",
      "loss: [0.68805635]\n",
      "0.504\n",
      "loss: [0.688055]\n",
      "0.506\n",
      "loss: [0.6880546]\n",
      "0.504\n",
      "loss: [0.6880526]\n",
      "0.504\n",
      "loss: [0.68805414]\n",
      "0.508\n",
      "loss: [0.6880531]\n",
      "0.502\n",
      "loss: [0.6880533]\n",
      "0.506\n",
      "loss: [0.6880518]\n",
      "0.506\n",
      "loss: [0.6880514]\n",
      "0.504\n",
      "loss: [0.68805104]\n",
      "0.504\n",
      "loss: [0.6880504]\n",
      "0.506\n",
      "loss: [0.68805087]\n",
      "0.504\n",
      "loss: [0.6880499]\n",
      "0.506\n",
      "loss: [0.68805027]\n",
      "0.504\n",
      "loss: [0.68804955]\n",
      "0.506\n",
      "loss: [0.68805003]\n",
      "0.504\n",
      "loss: [0.6880504]\n",
      "0.506\n",
      "loss: [0.6880535]\n",
      "0.504\n",
      "loss: [0.6880558]\n",
      "0.506\n",
      "loss: [0.6880577]\n",
      "0.504\n",
      "loss: [0.68805826]\n",
      "0.506\n",
      "loss: [0.6880584]\n",
      "0.504\n",
      "loss: [0.68805724]\n",
      "0.506\n",
      "loss: [0.6880562]\n",
      "0.504\n",
      "loss: [0.6880564]\n",
      "0.504\n",
      "loss: [0.6880562]\n",
      "0.506\n",
      "loss: [0.6880565]\n",
      "0.504\n",
      "loss: [0.6880561]\n",
      "0.502\n",
      "loss: [0.6880568]\n",
      "0.51\n",
      "loss: [0.68805474]\n",
      "0.502\n",
      "loss: [0.6880542]\n",
      "0.508\n",
      "loss: [0.68805164]\n",
      "0.504\n",
      "loss: [0.68805075]\n",
      "0.504\n",
      "loss: [0.6880494]\n",
      "0.508\n",
      "loss: [0.6880491]\n",
      "0.508\n",
      "loss: [0.68804944]\n",
      "0.504\n",
      "loss: [0.68805]\n",
      "0.506\n",
      "loss: [0.6880545]\n",
      "0.508\n",
      "loss: [0.6880584]\n",
      "0.506\n",
      "loss: [0.68806416]\n",
      "0.51\n",
      "loss: [0.68806493]\n",
      "0.51\n",
      "loss: [0.68806714]\n",
      "0.51\n",
      "loss: [0.6880639]\n",
      "0.506\n",
      "loss: [0.6880624]\n",
      "0.512\n",
      "loss: [0.68805873]\n",
      "0.506\n",
      "loss: [0.6880583]\n",
      "0.504\n",
      "loss: [0.68805736]\n",
      "0.506\n",
      "loss: [0.6880566]\n",
      "0.506\n",
      "loss: [0.68805647]\n",
      "0.504\n",
      "loss: [0.6880575]\n",
      "0.502\n",
      "loss: [0.6880601]\n",
      "0.51\n",
      "loss: [0.68805957]\n",
      "0.506\n",
      "loss: [0.68806064]\n",
      "0.508\n",
      "loss: [0.68806195]\n",
      "0.506\n",
      "loss: [0.688065]\n",
      "0.508\n",
      "loss: [0.68806654]\n",
      "0.504\n",
      "loss: [0.6880701]\n",
      "0.51\n",
      "loss: [0.6880711]\n",
      "0.504\n",
      "loss: [0.6880733]\n",
      "0.51\n",
      "loss: [0.68807]\n",
      "0.506\n",
      "loss: [0.6880681]\n",
      "0.508\n",
      "loss: [0.6880645]\n",
      "0.506\n",
      "loss: [0.6880623]\n",
      "0.508\n",
      "loss: [0.6880613]\n",
      "0.504\n",
      "loss: [0.68805957]\n",
      "0.502\n",
      "loss: [0.68805975]\n",
      "0.51\n",
      "loss: [0.6880574]\n",
      "0.502\n",
      "loss: [0.68805707]\n",
      "0.508\n",
      "loss: [0.6880547]\n",
      "0.504\n",
      "loss: [0.68805426]\n",
      "0.502\n",
      "loss: [0.6880534]\n",
      "0.508\n",
      "loss: [0.68805355]\n",
      "0.508\n",
      "loss: [0.6880543]\n",
      "0.502\n",
      "loss: [0.6880545]\n",
      "0.506\n",
      "loss: [0.6880589]\n",
      "0.508\n",
      "loss: [0.6880605]\n",
      "0.506\n",
      "loss: [0.68806344]\n",
      "0.508\n",
      "loss: [0.68806154]\n",
      "0.506\n",
      "loss: [0.68806124]\n",
      "0.51\n",
      "loss: [0.6880576]\n",
      "0.502\n",
      "loss: [0.688056]\n",
      "0.504\n",
      "loss: [0.6880537]\n",
      "0.508\n",
      "loss: [0.68805414]\n",
      "0.506\n",
      "loss: [0.6880543]\n",
      "0.506\n",
      "loss: [0.688054]\n",
      "0.508\n",
      "loss: [0.6880536]\n",
      "0.504\n",
      "loss: [0.6880524]\n",
      "0.506\n",
      "loss: [0.6880517]\n",
      "0.504\n",
      "loss: [0.6880503]\n",
      "0.506\n",
      "loss: [0.68804973]\n",
      "0.504\n",
      "loss: [0.68804884]\n",
      "0.508\n",
      "loss: [0.68804854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504\n",
      "loss: [0.6880483]\n",
      "0.508\n",
      "loss: [0.68804854]\n",
      "0.504\n",
      "loss: [0.68804944]\n",
      "0.508\n",
      "loss: [0.6880508]\n",
      "0.506\n",
      "loss: [0.68805164]\n",
      "0.506\n",
      "loss: [0.6880555]\n",
      "0.504\n",
      "loss: [0.6880576]\n",
      "0.502\n",
      "loss: [0.6880609]\n",
      "0.512\n",
      "loss: [0.68806046]\n",
      "0.506\n",
      "loss: [0.6880643]\n",
      "0.508\n",
      "loss: [0.688065]\n",
      "0.514\n",
      "loss: [0.688068]\n",
      "0.506\n",
      "loss: [0.68806505]\n",
      "0.514\n",
      "loss: [0.6880648]\n",
      "0.51\n",
      "loss: [0.68806]\n",
      "0.506\n",
      "loss: [0.68805814]\n",
      "0.51\n",
      "loss: [0.68805474]\n",
      "0.506\n",
      "loss: [0.6880537]\n",
      "0.504\n",
      "loss: [0.68805367]\n",
      "0.502\n",
      "loss: [0.6880538]\n",
      "0.506\n",
      "loss: [0.688056]\n",
      "0.508\n",
      "loss: [0.6880562]\n",
      "0.502\n",
      "loss: [0.6880585]\n",
      "0.512\n",
      "loss: [0.68805784]\n",
      "0.502\n",
      "loss: [0.6880592]\n",
      "0.508\n",
      "loss: [0.68805826]\n",
      "0.506\n",
      "loss: [0.68805856]\n",
      "0.502\n",
      "loss: [0.6880579]\n",
      "0.508\n",
      "loss: [0.68806]\n",
      "0.504\n",
      "loss: [0.68806094]\n",
      "0.506\n",
      "loss: [0.6880614]\n",
      "0.504\n",
      "loss: [0.6880604]\n",
      "0.506\n",
      "loss: [0.68805933]\n",
      "0.504\n",
      "loss: [0.6880575]\n",
      "0.506\n",
      "loss: [0.6880576]\n",
      "0.504\n",
      "loss: [0.68805903]\n",
      "0.502\n",
      "loss: [0.6880616]\n",
      "0.512\n",
      "loss: [0.68806064]\n",
      "0.506\n",
      "loss: [0.6880617]\n",
      "0.508\n",
      "loss: [0.6880585]\n",
      "0.506\n",
      "loss: [0.68805796]\n",
      "0.508\n",
      "loss: [0.68805397]\n",
      "0.506\n",
      "loss: [0.68805295]\n",
      "0.51\n",
      "loss: [0.688053]\n",
      "0.504\n",
      "loss: [0.6880555]\n",
      "0.51\n",
      "loss: [0.6880578]\n",
      "0.506\n",
      "loss: [0.68806255]\n",
      "0.51\n",
      "loss: [0.68806607]\n",
      "0.504\n",
      "loss: [0.6880716]\n",
      "0.51\n",
      "loss: [0.68807083]\n",
      "0.504\n",
      "loss: [0.6880717]\n",
      "0.508\n",
      "loss: [0.68806803]\n",
      "0.504\n",
      "loss: [0.6880663]\n",
      "0.506\n",
      "loss: [0.68806285]\n",
      "0.506\n",
      "loss: [0.68806064]\n",
      "0.508\n",
      "loss: [0.68805933]\n",
      "0.504\n",
      "loss: [0.6880573]\n",
      "0.502\n",
      "loss: [0.6880574]\n",
      "0.508\n",
      "loss: [0.6880558]\n",
      "0.502\n",
      "loss: [0.68806]\n",
      "0.51\n",
      "loss: [0.68806094]\n",
      "0.506\n",
      "loss: [0.6880639]\n",
      "0.508\n",
      "loss: [0.6880627]\n",
      "0.506\n",
      "loss: [0.6880654]\n",
      "0.512\n",
      "loss: [0.68806434]\n",
      "0.502\n",
      "loss: [0.68806404]\n",
      "0.504\n",
      "loss: [0.6880616]\n",
      "0.506\n",
      "loss: [0.6880595]\n",
      "0.506\n",
      "loss: [0.68805736]\n",
      "0.504\n",
      "loss: [0.6880558]\n",
      "0.506\n",
      "loss: [0.6880558]\n",
      "0.508\n",
      "loss: [0.6880539]\n",
      "0.502\n",
      "loss: [0.6880543]\n",
      "0.51\n",
      "loss: [0.68805206]\n",
      "0.504\n",
      "loss: [0.6880523]\n",
      "0.508\n",
      "loss: [0.68805164]\n",
      "0.502\n",
      "loss: [0.68805283]\n",
      "0.506\n",
      "loss: [0.6880523]\n",
      "0.506\n",
      "loss: [0.688053]\n",
      "0.504\n",
      "loss: [0.68805295]\n",
      "0.508\n",
      "loss: [0.68805325]\n",
      "0.508\n",
      "loss: [0.68805367]\n",
      "0.504\n",
      "loss: [0.6880534]\n",
      "0.506\n",
      "loss: [0.68805397]\n",
      "0.504\n",
      "loss: [0.6880532]\n",
      "0.506\n",
      "loss: [0.68805355]\n",
      "0.504\n",
      "loss: [0.68805265]\n",
      "0.506\n",
      "loss: [0.6880563]\n",
      "0.506\n",
      "loss: [0.6880576]\n",
      "0.502\n",
      "loss: [0.6880604]\n",
      "0.512\n",
      "loss: [0.68805975]\n",
      "0.506\n",
      "loss: [0.6880637]\n",
      "0.51\n",
      "loss: [0.688063]\n",
      "0.508\n",
      "loss: [0.68806434]\n",
      "0.508\n",
      "loss: [0.688061]\n",
      "0.506\n",
      "loss: [0.6880598]\n",
      "0.51\n",
      "loss: [0.6880558]\n",
      "0.502\n",
      "loss: [0.68805414]\n",
      "0.504\n",
      "loss: [0.68805176]\n",
      "0.508\n",
      "loss: [0.6880508]\n",
      "0.506\n",
      "loss: [0.6880507]\n",
      "0.502\n",
      "loss: [0.68805045]\n",
      "0.506\n",
      "loss: [0.6880518]\n",
      "0.506\n",
      "loss: [0.6880517]\n",
      "0.504\n",
      "loss: [0.6880534]\n",
      "0.508\n",
      "loss: [0.6880533]\n",
      "0.502\n",
      "loss: [0.68805796]\n",
      "0.512\n",
      "loss: [0.68805975]\n",
      "0.506\n",
      "loss: [0.68806535]\n",
      "0.508\n",
      "loss: [0.68806595]\n",
      "0.506\n",
      "loss: [0.6880681]\n",
      "0.51\n",
      "loss: [0.68806493]\n",
      "0.506\n",
      "loss: [0.68806386]\n",
      "0.51\n",
      "loss: [0.68805945]\n",
      "0.502\n",
      "loss: [0.6880572]\n",
      "0.504\n",
      "loss: [0.6880544]\n",
      "0.508\n",
      "loss: [0.68805283]\n",
      "0.508\n",
      "loss: [0.6880525]\n",
      "0.504\n",
      "loss: [0.68805176]\n",
      "0.506\n",
      "loss: [0.6880532]\n",
      "0.508\n",
      "loss: [0.68805546]\n",
      "0.508\n",
      "loss: [0.6880601]\n",
      "0.508\n",
      "loss: [0.6880605]\n",
      "0.508\n",
      "loss: [0.6880629]\n",
      "0.508\n",
      "loss: [0.6880609]\n",
      "0.506\n",
      "loss: [0.68806094]\n",
      "0.51\n",
      "loss: [0.6880584]\n",
      "0.504\n",
      "loss: [0.6880574]\n",
      "0.502\n",
      "loss: [0.6880561]\n",
      "0.506\n",
      "loss: [0.68805975]\n",
      "0.506\n",
      "loss: [0.6880636]\n",
      "0.504\n",
      "loss: [0.68806744]\n",
      "0.502\n",
      "loss: [0.6880729]\n",
      "0.508\n",
      "loss: [0.688077]\n",
      "0.514\n",
      "loss: [0.688087]\n",
      "0.514\n",
      "loss: [0.6880874]\n",
      "0.51\n",
      "loss: [0.68808854]\n",
      "0.514\n",
      "loss: [0.6880791]\n",
      "0.512\n",
      "loss: [0.6880726]\n",
      "0.51\n",
      "loss: [0.688064]\n",
      "0.506\n",
      "loss: [0.68806]\n",
      "0.506\n",
      "loss: [0.6880597]\n",
      "0.508\n",
      "loss: [0.68806005]\n",
      "0.504\n",
      "loss: [0.6880638]\n",
      "0.508\n",
      "loss: [0.68806267]\n",
      "0.51\n",
      "loss: [0.68806374]\n",
      "0.508\n",
      "loss: [0.68805975]\n",
      "0.504\n",
      "loss: [0.6880596]\n",
      "0.504\n",
      "loss: [0.6880605]\n",
      "0.504\n",
      "loss: [0.6880623]\n",
      "0.502\n",
      "loss: [0.68806607]\n",
      "0.51\n",
      "loss: [0.6880651]\n",
      "0.506\n",
      "loss: [0.6880655]\n",
      "0.51\n",
      "loss: [0.6880621]\n",
      "0.502\n",
      "loss: [0.6880609]\n",
      "0.502\n",
      "loss: [0.68805987]\n",
      "0.504\n",
      "loss: [0.6880594]\n",
      "0.506\n",
      "loss: [0.68806046]\n",
      "0.508\n",
      "loss: [0.6880589]\n",
      "0.502\n",
      "loss: [0.68805885]\n",
      "0.508\n",
      "loss: [0.6880561]\n",
      "0.506\n",
      "loss: [0.68805486]\n",
      "0.502\n",
      "loss: [0.6880534]\n",
      "0.506\n",
      "loss: [0.6880523]\n",
      "0.506\n",
      "loss: [0.6880522]\n",
      "0.504\n",
      "loss: [0.68805134]\n",
      "0.506\n",
      "loss: [0.6880567]\n",
      "0.512\n",
      "loss: [0.68805885]\n",
      "0.506\n",
      "loss: [0.688062]\n",
      "0.508\n",
      "loss: [0.68805987]\n",
      "0.506\n",
      "loss: [0.6880591]\n",
      "0.504\n",
      "loss: [0.6880564]\n",
      "0.506\n",
      "loss: [0.6880548]\n",
      "0.508\n",
      "loss: [0.68805414]\n",
      "0.504\n",
      "loss: [0.6880523]\n",
      "0.502\n",
      "loss: [0.68805224]\n",
      "0.51\n",
      "loss: [0.68804973]\n",
      "0.502\n",
      "loss: [0.6880494]\n",
      "0.506\n",
      "loss: [0.68804777]\n",
      "0.506\n",
      "loss: [0.688048]\n",
      "0.504\n",
      "loss: [0.6880488]\n",
      "0.502\n",
      "loss: [0.6880496]\n",
      "0.506\n",
      "loss: [0.68805164]\n",
      "0.504\n",
      "loss: [0.68805206]\n",
      "0.506\n",
      "loss: [0.68805355]\n",
      "0.504\n",
      "loss: [0.68805325]\n",
      "0.506\n",
      "loss: [0.6880537]\n",
      "0.506\n",
      "loss: [0.68805367]\n",
      "0.508\n",
      "loss: [0.68805605]\n",
      "0.504\n",
      "loss: [0.68805754]\n",
      "0.506\n",
      "loss: [0.68806106]\n",
      "0.504\n",
      "loss: [0.6880621]\n",
      "0.502\n",
      "loss: [0.6880632]\n",
      "0.504\n",
      "loss: [0.6880611]\n",
      "0.502\n",
      "loss: [0.6880596]\n",
      "0.504\n",
      "loss: [0.6880563]\n",
      "0.506\n",
      "loss: [0.68805397]\n",
      "0.504\n",
      "loss: [0.6880516]\n",
      "0.506\n",
      "loss: [0.6880497]\n",
      "0.508\n",
      "loss: [0.68804896]\n",
      "0.502\n",
      "loss: [0.6880478]\n",
      "0.506\n",
      "loss: [0.6880484]\n",
      "0.504\n",
      "loss: [0.68805015]\n",
      "0.504\n",
      "loss: [0.68805337]\n",
      "0.508\n",
      "loss: [0.68805397]\n",
      "0.502\n",
      "loss: [0.6880592]\n",
      "0.512\n",
      "loss: [0.6880608]\n",
      "0.506\n",
      "loss: [0.6880652]\n",
      "0.51\n",
      "loss: [0.68806463]\n",
      "0.506\n",
      "loss: [0.68806505]\n",
      "0.51\n",
      "loss: [0.68806136]\n",
      "0.502\n",
      "loss: [0.6880591]\n",
      "0.504\n",
      "loss: [0.68805563]\n",
      "0.508\n",
      "loss: [0.68805337]\n",
      "0.508\n",
      "loss: [0.68805224]\n",
      "0.502\n",
      "loss: [0.68805104]\n",
      "0.506\n",
      "loss: [0.6880519]\n",
      "0.508\n",
      "loss: [0.6880514]\n",
      "0.502\n",
      "loss: [0.68805295]\n",
      "0.508\n",
      "loss: [0.6880559]\n",
      "0.502\n",
      "loss: [0.6880606]\n",
      "0.51\n",
      "loss: [0.6880643]\n",
      "0.506\n",
      "loss: [0.68807]\n",
      "0.51\n",
      "loss: [0.68807495]\n",
      "0.512\n",
      "loss: [0.68808144]\n",
      "0.51\n",
      "loss: [0.6880801]\n",
      "0.51\n",
      "loss: [0.6880796]\n",
      "0.508\n",
      "loss: [0.68807214]\n",
      "0.508\n",
      "loss: [0.68806744]\n",
      "0.508\n",
      "loss: [0.68806213]\n",
      "0.508\n",
      "loss: [0.6880591]\n",
      "0.506\n",
      "loss: [0.6880588]\n",
      "0.51\n",
      "loss: [0.68805695]\n",
      "0.506\n",
      "loss: [0.6880581]\n",
      "0.508\n",
      "loss: [0.68805593]\n",
      "0.506\n",
      "loss: [0.6880568]\n",
      "0.51\n",
      "loss: [0.6880561]\n",
      "0.506\n",
      "loss: [0.68805754]\n",
      "0.504\n",
      "loss: [0.6880594]\n",
      "0.504\n",
      "loss: [0.6880606]\n",
      "0.506\n",
      "loss: [0.6880625]\n",
      "0.508\n",
      "loss: [0.68806136]\n",
      "0.502\n",
      "loss: [0.68806106]\n",
      "0.508\n",
      "loss: [0.68805784]\n",
      "0.504\n",
      "loss: [0.6880561]\n",
      "0.504\n",
      "loss: [0.6880535]\n",
      "0.508\n",
      "loss: [0.6880519]\n",
      "0.508\n",
      "loss: [0.6880511]\n",
      "0.502\n",
      "loss: [0.68805045]\n",
      "0.506\n",
      "loss: [0.68805426]\n",
      "0.508\n",
      "loss: [0.68805647]\n",
      "0.506\n",
      "loss: [0.6880628]\n",
      "0.508\n",
      "loss: [0.6880639]\n",
      "0.51\n",
      "loss: [0.68806624]\n",
      "0.51\n",
      "loss: [0.68806255]\n",
      "0.506\n",
      "loss: [0.6880607]\n",
      "0.508\n",
      "loss: [0.68805665]\n",
      "0.506\n",
      "loss: [0.68805414]\n",
      "0.508\n",
      "loss: [0.68805265]\n",
      "0.504\n",
      "loss: [0.6880507]\n",
      "0.502\n",
      "loss: [0.688051]\n",
      "0.51\n",
      "loss: [0.6880491]\n",
      "0.504\n",
      "loss: [0.6880498]\n",
      "0.51\n",
      "loss: [0.68804866]\n",
      "0.504\n",
      "loss: [0.6880516]\n",
      "0.504\n",
      "loss: [0.688053]\n",
      "0.506\n",
      "loss: [0.6880578]\n",
      "0.504\n",
      "loss: [0.68806005]\n",
      "0.506\n",
      "loss: [0.6880621]\n",
      "0.504\n",
      "loss: [0.6880614]\n",
      "0.506\n",
      "loss: [0.6880605]\n",
      "0.506\n",
      "loss: [0.68805844]\n",
      "0.508\n",
      "loss: [0.6880563]\n",
      "0.508\n",
      "loss: [0.6880543]\n",
      "0.504\n",
      "loss: [0.6880522]\n",
      "0.506\n",
      "loss: [0.68805116]\n",
      "0.502\n",
      "loss: [0.68804973]\n",
      "0.506\n",
      "loss: [0.6880497]\n",
      "0.502\n",
      "loss: [0.68804926]\n",
      "0.506\n",
      "loss: [0.6880499]\n",
      "0.502\n",
      "loss: [0.68805027]\n",
      "0.508\n",
      "loss: [0.68805116]\n",
      "0.504\n",
      "loss: [0.6880534]\n",
      "0.508\n",
      "loss: [0.6880555]\n",
      "0.504\n",
      "loss: [0.68805623]\n",
      "0.506\n",
      "loss: [0.6880569]\n",
      "0.504\n",
      "loss: [0.6880558]\n",
      "0.506\n",
      "loss: [0.6880553]\n",
      "0.504\n",
      "loss: [0.6880535]\n",
      "0.506\n",
      "loss: [0.68805254]\n",
      "0.502\n",
      "loss: [0.6880511]\n",
      "0.506\n",
      "loss: [0.68805325]\n",
      "0.504\n",
      "loss: [0.6880547]\n",
      "0.504\n",
      "loss: [0.6880595]\n",
      "0.51\n",
      "loss: [0.68806535]\n",
      "0.508\n",
      "loss: [0.6880743]\n",
      "0.508\n",
      "loss: [0.6880836]\n",
      "0.512\n",
      "loss: [0.68809694]\n",
      "0.512\n",
      "loss: [0.6881006]\n",
      "0.512\n",
      "loss: [0.68810356]\n",
      "0.51\n",
      "loss: [0.68808913]\n",
      "0.51\n",
      "loss: [0.6880787]\n",
      "0.51\n",
      "loss: [0.68807]\n",
      "0.506\n",
      "loss: [0.68806934]\n",
      "0.506\n",
      "loss: [0.68807524]\n",
      "0.508\n",
      "loss: [0.688074]\n",
      "0.512\n",
      "loss: [0.6880736]\n",
      "0.51\n",
      "loss: [0.6880657]\n",
      "0.506\n",
      "loss: [0.68806404]\n",
      "0.504\n",
      "loss: [0.6880667]\n",
      "0.506\n",
      "loss: [0.6880705]\n",
      "0.504\n",
      "loss: [0.6880762]\n",
      "0.508\n",
      "loss: [0.6880741]\n",
      "0.504\n",
      "loss: [0.6880733]\n",
      "0.51\n",
      "loss: [0.6880706]\n",
      "0.508\n",
      "loss: [0.68807]\n",
      "0.506\n",
      "loss: [0.68807113]\n",
      "0.512\n"
     ]
    }
   ],
   "source": [
    "n = g.number_of_nodes()\n",
    "n_hidden = 16\n",
    "n_embedding_updates = 1\n",
    "n_parameter_updates = 1\n",
    "alpha = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-2\n",
    "\n",
    "g.ndata['x'] = mx.nd.eye(n, n)\n",
    "#g.ndata['x'] = mx.nd.arange(n).reshape(n, 1)\n",
    "g.ndata['h'] = mx.nd.random.normal(shape=(n, n_hidden))\n",
    "\n",
    "steady_state_operator = SteadyStateOperator(n_hidden, 'relu')\n",
    "steady_state_operator.initialize()\n",
    "predictor = Predictor(n_hidden, 'relu')\n",
    "predictor.initialize()\n",
    "trainer = gluon.Trainer(predictor.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "def update_parameters(g, trainer):\n",
    "    with mx.autograd.record():\n",
    "        steady_state_operator(g)\n",
    "        z = predictor(g)\n",
    "        loss = loss_fcn(z, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "        print(\"loss: \" + str(loss.asnumpy()))\n",
    "    loss.backward()\n",
    "    trainer.step(1)  # divide gradients by the number of labelled nodes\n",
    "    return loss.asnumpy()[0]\n",
    "\n",
    "def train(g, trainer):\n",
    "     # first phase\n",
    "    for i in range(n_embedding_updates):\n",
    "        update_embeddings(g, steady_state_operator)\n",
    "    # second phase\n",
    "    for i in range(n_parameter_updates):\n",
    "        loss = update_parameters(g, trainer)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train(g, trainer)\n",
    "    acc = evaluate(g.ndata['z'], labels, eval_mask)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
