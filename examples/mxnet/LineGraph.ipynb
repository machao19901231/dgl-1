{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'mxnet'\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import math\n",
    "import numpy as np\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoint_chains(n_chains, length):\n",
    "    path_graph = nx.path_graph(n_chains * length).to_directed()\n",
    "    for i in range(n_chains - 1):  # break the path graph into N chains\n",
    "        path_graph.remove_edge((i + 1) * length - 1, (i + 1) * length)\n",
    "        path_graph.remove_edge((i + 1) * length, (i + 1) * length - 1)\n",
    "    for n in path_graph.nodes:\n",
    "        path_graph.add_edge(n, n)  # add self connections\n",
    "    return path_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = disjoint_chains(1, 30)\n",
    "nx.draw(g1, pos=nx.circular_layout(g1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g2 = disjoint_chains(2, 15)\n",
    "nx.draw(g2, pos=nx.circular_layout(g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 5000\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "\n",
    "g = dgl.DGLGraph(disjoint_chains(1, 10000), readonly=True)\n",
    "labels = mx.nd.zeros([g.number_of_nodes()])\n",
    "labels[mx.nd.arange(0, g.number_of_nodes(), 2)] = 1\n",
    "pattern = [0, 1, 1, 0, 1, 1, 1, 0]\n",
    "for i in range(int(g.number_of_nodes()/len(pattern))):\n",
    "    labels[i*len(pattern):(i + 1) * len(pattern)] = pattern\n",
    "\n",
    "train_mask = mx.nd.zeros((g.number_of_nodes()))\n",
    "train_mask[0:n_train_samples] = 1\n",
    "eval_mask = mx.nd.zeros((g.number_of_nodes()))\n",
    "eval_mask[n_train_samples:] = 1\n",
    "\n",
    "\n",
    "def evaluate(pred, labels, mask):\n",
    "    pred = pred.argmax(axis=1)\n",
    "    accuracy = ((pred == labels) * mask).sum() / mask.sum().asscalar()\n",
    "    return accuracy.asscalar()\n",
    "\n",
    "# Helper function to convert a number \n",
    "# to its fixed width binary representation\n",
    "def conv(x):\n",
    "  a = format(x, '032b')\n",
    "  l = list(str(a))\n",
    "  l = np.array(list(map(int, l)))\n",
    "  return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we predict with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.6131182], acc: 0.6486\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 16\n",
    "n_classes = 2\n",
    "\n",
    "class MLP(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "        self.dense2 = gluon.nn.Dense(n_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        return self.dense2(self.dense1(h))\n",
    "    \n",
    "model = MLP(n_hidden, n_classes, 'relu')\n",
    "model.initialize()\n",
    "features = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(100):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "print(\"loss: \" + str(loss.asnumpy()) + \", acc: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we define a GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 16\n",
    "n_classes = 2\n",
    "\n",
    "class GCNLayer(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 activation):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.dense = gluon.nn.Dense(out_feats, activation=activation)\n",
    "\n",
    "    def forward(self, h):\n",
    "        self.g.ndata['h'] = h\n",
    "        def concat_msg(edges):\n",
    "            return {'m': edges.src['h']}\n",
    "        def red_func(nodes):\n",
    "            m = nodes.mailbox['m']\n",
    "            if m.shape[1] == 3:\n",
    "                h = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(h, nodes.data['h'], dim=1)\n",
    "            else:\n",
    "                num_feats = m.shape[2]\n",
    "                m = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(m, nodes.data['h'], mx.nd.zeros(shape=(m.shape[0], num_feats)), dim=1)\n",
    "            return {'h': self.dense(h)}\n",
    "        self.g.update_all(concat_msg, red_func)\n",
    "        return mx.nd.concat(self.g.ndata.pop('h'), h, dim=1)\n",
    "\n",
    "class GCN(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = gluon.nn.Sequential()\n",
    "        # input layer\n",
    "        self.layers.add(GCNLayer(g, in_feats, n_hidden, activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.add(GCNLayer(g, n_hidden, n_hidden, activation))\n",
    "        self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "        self.dense2 = gluon.nn.Dense(n_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        return self.dense2(self.dense1(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degs = g.in_degrees().astype('float32')\n",
    "norm = mx.nd.power(degs, -0.5)\n",
    "g.ndata['norm'] = mx.nd.expand_dims(norm, 1)\n",
    "features = mx.nd.array([conv(i) for i in range(g.number_of_nodes())])\n",
    "model = GCN(g, in_feats=features.shape[1], n_hidden=16, n_classes=2, n_layers=2, activation='relu', dropout=0.5)\n",
    "model.initialize()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "            {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "h = model(features)\n",
    "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
    "for epoch in range(100):\n",
    "    with mx.autograd.record():\n",
    "        pred = model(features)\n",
    "        loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    acc = evaluate(pred, labels, eval_mask)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here we define an SSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteadyStateOperator(gluon.Block):\n",
    "    def __init__(self, n_hidden, activation, **kwargs):\n",
    "        super(SteadyStateOperator, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "            self.dense2 = gluon.nn.Dense(n_hidden)\n",
    "\n",
    "    def forward(self, g):\n",
    "        def concat_msg(edges):\n",
    "            return {'m': edges.src['xh']}\n",
    "        def red_func(nodes):\n",
    "            m = nodes.mailbox['m']\n",
    "            if m.shape[1] == 3:\n",
    "                h = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(h, nodes.data['h'], dim=1)\n",
    "            else:\n",
    "                num_feats = m.shape[2]\n",
    "                m = m.reshape(m.shape[0], m.shape[1] * m.shape[2])\n",
    "                h = mx.nd.concat(m, nodes.data['h'], mx.nd.zeros(shape=(m.shape[0], num_feats)), dim=1)\n",
    "            return {'h': self.dense2(self.dense1(h))}\n",
    "        \n",
    "        g.ndata['xh'] = mx.nd.concat(g.ndata['x'], g.ndata['h'], dim=1)\n",
    "        g.update_all(concat_msg, red_func)\n",
    "        return g.ndata['h']\n",
    "\n",
    "def update_embeddings(g, steady_state_operator):\n",
    "    prev_h = g.ndata['h']\n",
    "    next_h = steady_state_operator(g)\n",
    "    g.ndata['h'] = (1 - alpha) * prev_h + alpha * next_h\n",
    "    \n",
    "class Predictor(gluon.Block):\n",
    "    def __init__(self, n_hidden, activation, **kwargs):\n",
    "        super(Predictor, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n",
    "            self.dense2 = gluon.nn.Dense(2)  ## binary classifier\n",
    "\n",
    "    def forward(self, g):\n",
    "        g.ndata['z'] = self.dense2(self.dense1(g.ndata['h']))\n",
    "        return g.ndata['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.6929703]\n",
      "0.494\n",
      "loss: [0.67982626]\n",
      "0.532\n",
      "loss: [0.70458907]\n",
      "0.5\n",
      "loss: [0.6316663]\n",
      "0.724\n",
      "loss: [0.6384644]\n",
      "0.504\n",
      "loss: [0.56882256]\n",
      "0.716\n",
      "loss: [0.5465602]\n",
      "0.838\n",
      "loss: [0.4808145]\n",
      "0.85\n",
      "loss: [0.4369374]\n",
      "0.736\n",
      "loss: [0.3985014]\n",
      "0.734\n",
      "loss: [0.34531602]\n",
      "0.816\n",
      "loss: [0.3257199]\n",
      "0.848\n",
      "loss: [0.27859727]\n",
      "0.8\n",
      "loss: [0.26783895]\n",
      "0.746\n",
      "loss: [0.22862947]\n",
      "0.79\n",
      "loss: [0.2198457]\n",
      "0.82\n",
      "loss: [0.193969]\n",
      "0.806\n",
      "loss: [0.18921678]\n",
      "0.752\n",
      "loss: [0.17162174]\n",
      "0.762\n",
      "loss: [0.1659842]\n",
      "0.792\n",
      "loss: [0.15364626]\n",
      "0.784\n",
      "loss: [0.14967307]\n",
      "0.746\n",
      "loss: [0.13916418]\n",
      "0.75\n",
      "loss: [0.13439754]\n",
      "0.784\n",
      "loss: [0.12523524]\n",
      "0.778\n",
      "loss: [0.12346719]\n",
      "0.738\n",
      "loss: [0.11464119]\n",
      "0.754\n",
      "loss: [0.11391784]\n",
      "0.792\n",
      "loss: [0.1062059]\n",
      "0.776\n",
      "loss: [0.10783598]\n",
      "0.744\n",
      "loss: [0.10150047]\n",
      "0.784\n",
      "loss: [0.10280566]\n",
      "0.798\n",
      "loss: [0.10075304]\n",
      "0.768\n",
      "loss: [0.09871136]\n",
      "0.776\n",
      "loss: [0.10039452]\n",
      "0.808\n",
      "loss: [0.09680078]\n",
      "0.784\n",
      "loss: [0.09600893]\n",
      "0.78\n",
      "loss: [0.09788258]\n",
      "0.82\n",
      "loss: [0.09681068]\n",
      "0.764\n",
      "loss: [0.09481519]\n",
      "0.798\n",
      "loss: [0.09568119]\n",
      "0.808\n",
      "loss: [0.09818044]\n",
      "0.768\n",
      "loss: [0.09954698]\n",
      "0.83\n",
      "loss: [0.09960519]\n",
      "0.766\n",
      "loss: [0.09707197]\n",
      "0.836\n",
      "loss: [0.09415971]\n",
      "0.792\n",
      "loss: [0.09216835]\n",
      "0.81\n",
      "loss: [0.09200352]\n",
      "0.816\n",
      "loss: [0.09295861]\n",
      "0.796\n",
      "loss: [0.09349245]\n",
      "0.828\n",
      "loss: [0.09346604]\n",
      "0.77\n",
      "loss: [0.09165468]\n",
      "0.822\n",
      "loss: [0.08921181]\n",
      "0.77\n",
      "loss: [0.08639119]\n",
      "0.812\n",
      "loss: [0.08473148]\n",
      "0.798\n",
      "loss: [0.0841494]\n",
      "0.79\n",
      "loss: [0.08431526]\n",
      "0.808\n",
      "loss: [0.08546735]\n",
      "0.752\n",
      "loss: [0.08695719]\n",
      "0.812\n",
      "loss: [0.08985285]\n",
      "0.742\n",
      "loss: [0.09177107]\n",
      "0.824\n",
      "loss: [0.09164865]\n",
      "0.732\n",
      "loss: [0.08601616]\n",
      "0.814\n",
      "loss: [0.08008823]\n",
      "0.764\n",
      "loss: [0.07894508]\n",
      "0.768\n",
      "loss: [0.08222461]\n",
      "0.812\n",
      "loss: [0.08557075]\n",
      "0.73\n",
      "loss: [0.08439028]\n",
      "0.812\n",
      "loss: [0.08034581]\n",
      "0.744\n",
      "loss: [0.07684307]\n",
      "0.788\n",
      "loss: [0.07729261]\n",
      "0.796\n",
      "loss: [0.08017959]\n",
      "0.742\n",
      "loss: [0.08168432]\n",
      "0.81\n",
      "loss: [0.08084365]\n",
      "0.738\n",
      "loss: [0.07779724]\n",
      "0.802\n",
      "loss: [0.07582474]\n",
      "0.772\n",
      "loss: [0.07606356]\n",
      "0.768\n",
      "loss: [0.07765473]\n",
      "0.808\n",
      "loss: [0.07948419]\n",
      "0.742\n",
      "loss: [0.0798689]\n",
      "0.81\n",
      "loss: [0.0793646]\n",
      "0.742\n",
      "loss: [0.07751161]\n",
      "0.81\n",
      "loss: [0.07587396]\n",
      "0.75\n",
      "loss: [0.0746034]\n",
      "0.788\n",
      "loss: [0.07434085]\n",
      "0.788\n",
      "loss: [0.07486478]\n",
      "0.762\n",
      "loss: [0.07566785]\n",
      "0.802\n",
      "loss: [0.07702636]\n",
      "0.744\n",
      "loss: [0.07826912]\n",
      "0.81\n",
      "loss: [0.08014149]\n",
      "0.736\n",
      "loss: [0.08155874]\n",
      "0.816\n",
      "loss: [0.08276922]\n",
      "0.732\n",
      "loss: [0.08173746]\n",
      "0.816\n",
      "loss: [0.07876346]\n",
      "0.736\n",
      "loss: [0.07436346]\n",
      "0.802\n",
      "loss: [0.07206459]\n",
      "0.768\n",
      "loss: [0.07280393]\n",
      "0.754\n",
      "loss: [0.07483658]\n",
      "0.81\n",
      "loss: [0.0763285]\n",
      "0.738\n",
      "loss: [0.07534142]\n",
      "0.81\n",
      "loss: [0.07331505]\n",
      "0.746\n",
      "loss: [0.07136839]\n",
      "0.788\n",
      "loss: [0.07120873]\n",
      "0.782\n",
      "loss: [0.07249822]\n",
      "0.75\n",
      "loss: [0.07374459]\n",
      "0.806\n",
      "loss: [0.0745974]\n",
      "0.74\n",
      "loss: [0.07411473]\n",
      "0.806\n",
      "loss: [0.07331517]\n",
      "0.75\n",
      "loss: [0.07215727]\n",
      "0.798\n",
      "loss: [0.07161825]\n",
      "0.762\n",
      "loss: [0.07157715]\n",
      "0.766\n",
      "loss: [0.07197297]\n",
      "0.79\n",
      "loss: [0.07281081]\n",
      "0.756\n",
      "loss: [0.07363593]\n",
      "0.804\n",
      "loss: [0.07502069]\n",
      "0.74\n",
      "loss: [0.07643368]\n",
      "0.808\n",
      "loss: [0.07881743]\n",
      "0.736\n",
      "loss: [0.08125981]\n",
      "0.818\n",
      "loss: [0.0840892]\n",
      "0.726\n",
      "loss: [0.08470751]\n",
      "0.824\n",
      "loss: [0.08189417]\n",
      "0.73\n",
      "loss: [0.07524636]\n",
      "0.81\n",
      "loss: [0.07041629]\n",
      "0.764\n",
      "loss: [0.07116185]\n",
      "0.75\n",
      "loss: [0.07458036]\n",
      "0.81\n",
      "loss: [0.07573766]\n",
      "0.734\n",
      "loss: [0.07249372]\n",
      "0.808\n",
      "loss: [0.06937254]\n",
      "0.756\n",
      "loss: [0.06966151]\n",
      "0.756\n",
      "loss: [0.07198764]\n",
      "0.804\n",
      "loss: [0.07313034]\n",
      "0.74\n",
      "loss: [0.07136513]\n",
      "0.804\n",
      "loss: [0.06962763]\n",
      "0.762\n",
      "loss: [0.06976967]\n",
      "0.762\n",
      "loss: [0.07131048]\n",
      "0.802\n",
      "loss: [0.07263389]\n",
      "0.74\n",
      "loss: [0.07217479]\n",
      "0.804\n",
      "loss: [0.07118594]\n",
      "0.756\n",
      "loss: [0.07037859]\n",
      "0.782\n",
      "loss: [0.07063917]\n",
      "0.786\n",
      "loss: [0.07166199]\n",
      "0.758\n",
      "loss: [0.07244939]\n",
      "0.8\n",
      "loss: [0.07300522]\n",
      "0.746\n",
      "loss: [0.07266629]\n",
      "0.8\n",
      "loss: [0.07222246]\n",
      "0.758\n",
      "loss: [0.07150867]\n",
      "0.79\n",
      "loss: [0.07118423]\n",
      "0.762\n",
      "loss: [0.07113457]\n",
      "0.766\n",
      "loss: [0.07131755]\n",
      "0.788\n",
      "loss: [0.07174072]\n",
      "0.758\n",
      "loss: [0.07198954]\n",
      "0.794\n",
      "loss: [0.07239443]\n",
      "0.75\n",
      "loss: [0.07245354]\n",
      "0.8\n",
      "loss: [0.0726946]\n",
      "0.746\n",
      "loss: [0.07255814]\n",
      "0.802\n",
      "loss: [0.07262325]\n",
      "0.744\n",
      "loss: [0.0723184]\n",
      "0.8\n",
      "loss: [0.07223033]\n",
      "0.75\n",
      "loss: [0.07182451]\n",
      "0.8\n",
      "loss: [0.0716604]\n",
      "0.754\n",
      "loss: [0.07126524]\n",
      "0.796\n",
      "loss: [0.07112664]\n",
      "0.756\n",
      "loss: [0.07083187]\n",
      "0.79\n",
      "loss: [0.07077669]\n",
      "0.756\n",
      "loss: [0.07060415]\n",
      "0.788\n",
      "loss: [0.07064445]\n",
      "0.758\n",
      "loss: [0.07058094]\n",
      "0.788\n",
      "loss: [0.07073212]\n",
      "0.758\n",
      "loss: [0.07078999]\n",
      "0.79\n",
      "loss: [0.07114691]\n",
      "0.756\n",
      "loss: [0.07148153]\n",
      "0.8\n",
      "loss: [0.072437]\n",
      "0.742\n",
      "loss: [0.07375763]\n",
      "0.804\n",
      "loss: [0.07678744]\n",
      "0.736\n",
      "loss: [0.0818146]\n",
      "0.82\n",
      "loss: [0.09150162]\n",
      "0.706\n",
      "loss: [0.10602666]\n",
      "0.848\n",
      "loss: [0.12114193]\n",
      "0.662\n",
      "loss: [0.11654776]\n",
      "0.856\n",
      "loss: [0.08133347]\n",
      "0.724\n",
      "loss: [0.06983407]\n",
      "0.744\n",
      "loss: [0.09098017]\n",
      "0.836\n",
      "loss: [0.07811337]\n",
      "0.722\n",
      "loss: [0.06546238]\n",
      "0.754\n",
      "loss: [0.0800245]\n",
      "0.82\n",
      "loss: [0.07061295]\n",
      "0.73\n",
      "loss: [0.06555304]\n",
      "0.742\n",
      "loss: [0.07543014]\n",
      "0.818\n",
      "loss: [0.06639795]\n",
      "0.74\n",
      "loss: [0.06745139]\n",
      "0.74\n",
      "loss: [0.0729521]\n",
      "0.818\n",
      "loss: [0.06554956]\n",
      "0.76\n",
      "loss: [0.06981673]\n",
      "0.74\n",
      "loss: [0.07210857]\n",
      "0.812\n",
      "loss: [0.06677405]\n",
      "0.764\n",
      "loss: [0.07135517]\n",
      "0.738\n",
      "loss: [0.07252652]\n",
      "0.808\n",
      "loss: [0.06856114]\n",
      "0.764\n",
      "loss: [0.07199999]\n",
      "0.744\n",
      "loss: [0.07372098]\n",
      "0.806\n",
      "loss: [0.07059462]\n",
      "0.764\n",
      "loss: [0.07201007]\n",
      "0.758\n",
      "loss: [0.07442022]\n",
      "0.806\n",
      "loss: [0.07251371]\n",
      "0.762\n",
      "loss: [0.07193961]\n",
      "0.766\n",
      "loss: [0.07411347]\n",
      "0.806\n",
      "loss: [0.07374281]\n",
      "0.752\n",
      "loss: [0.07208794]\n",
      "0.784\n",
      "loss: [0.07310132]\n",
      "0.798\n",
      "loss: [0.07389835]\n",
      "0.746\n",
      "loss: [0.07243938]\n",
      "0.788\n",
      "loss: [0.0721953]\n",
      "0.788\n",
      "loss: [0.07322812]\n",
      "0.754\n",
      "loss: [0.07271948]\n",
      "0.794\n",
      "loss: [0.07189114]\n",
      "0.768\n",
      "loss: [0.07229861]\n",
      "0.758\n",
      "loss: [0.07256364]\n",
      "0.794\n",
      "loss: [0.07200106]\n",
      "0.76\n",
      "loss: [0.07166976]\n",
      "0.766\n",
      "loss: [0.07202167]\n",
      "0.79\n",
      "loss: [0.07203653]\n",
      "0.756\n",
      "loss: [0.07146153]\n",
      "0.782\n",
      "loss: [0.07144642]\n",
      "0.782\n",
      "loss: [0.07176182]\n",
      "0.756\n",
      "loss: [0.07150319]\n",
      "0.786\n",
      "loss: [0.07122958]\n",
      "0.764\n",
      "loss: [0.07131833]\n",
      "0.762\n",
      "loss: [0.07140711]\n",
      "0.786\n",
      "loss: [0.07130139]\n",
      "0.762\n",
      "loss: [0.07107326]\n",
      "0.772\n",
      "loss: [0.07110842]\n",
      "0.782\n",
      "loss: [0.07122833]\n",
      "0.762\n",
      "loss: [0.07104965]\n",
      "0.782\n",
      "loss: [0.07091499]\n",
      "0.766\n",
      "loss: [0.07092436]\n",
      "0.762\n",
      "loss: [0.07093342]\n",
      "0.784\n",
      "loss: [0.07090494]\n",
      "0.762\n",
      "loss: [0.07075232]\n",
      "0.776\n",
      "loss: [0.07071577]\n",
      "0.774\n",
      "loss: [0.07076991]\n",
      "0.762\n",
      "loss: [0.07071567]\n",
      "0.782\n",
      "loss: [0.07065953]\n",
      "0.762\n",
      "loss: [0.07058226]\n",
      "0.768\n",
      "loss: [0.07056509]\n",
      "0.778\n",
      "loss: [0.07058737]\n",
      "0.762\n",
      "loss: [0.07051131]\n",
      "0.78\n",
      "loss: [0.07046239]\n",
      "0.762\n",
      "loss: [0.07042012]\n",
      "0.762\n",
      "loss: [0.07040229]\n",
      "0.774\n",
      "loss: [0.07041413]\n",
      "0.762\n",
      "loss: [0.07035761]\n",
      "0.774\n",
      "loss: [0.07033044]\n",
      "0.762\n",
      "loss: [0.07030329]\n",
      "0.762\n",
      "loss: [0.0702873]\n",
      "0.772\n",
      "loss: [0.07029467]\n",
      "0.762\n",
      "loss: [0.07025255]\n",
      "0.772\n",
      "loss: [0.07023424]\n",
      "0.762\n",
      "loss: [0.07020465]\n",
      "0.766\n",
      "loss: [0.07018894]\n",
      "0.77\n",
      "loss: [0.07019362]\n",
      "0.762\n",
      "loss: [0.07016934]\n",
      "0.772\n",
      "loss: [0.07016716]\n",
      "0.762\n",
      "loss: [0.07014327]\n",
      "0.77\n",
      "loss: [0.07013632]\n",
      "0.768\n",
      "loss: [0.07013667]\n",
      "0.762\n",
      "loss: [0.07012585]\n",
      "0.772\n",
      "loss: [0.07013166]\n",
      "0.762\n",
      "loss: [0.07011231]\n",
      "0.77\n",
      "loss: [0.07011]\n",
      "0.762\n",
      "loss: [0.07009783]\n",
      "0.768\n",
      "loss: [0.07009169]\n",
      "0.768\n",
      "loss: [0.07009269]\n",
      "0.762\n",
      "loss: [0.0700819]\n",
      "0.77\n",
      "loss: [0.07008509]\n",
      "0.762\n",
      "loss: [0.07007003]\n",
      "0.77\n",
      "loss: [0.07006812]\n",
      "0.762\n",
      "loss: [0.07005534]\n",
      "0.768\n",
      "loss: [0.07004813]\n",
      "0.764\n",
      "loss: [0.07004127]\n",
      "0.762\n",
      "loss: [0.07002967]\n",
      "0.768\n",
      "loss: [0.07002661]\n",
      "0.762\n",
      "loss: [0.07001217]\n",
      "0.768\n",
      "loss: [0.07000975]\n",
      "0.762\n",
      "loss: [0.06999551]\n",
      "0.768\n",
      "loss: [0.06999223]\n",
      "0.762\n",
      "loss: [0.06998099]\n",
      "0.768\n",
      "loss: [0.06997639]\n",
      "0.764\n",
      "loss: [0.06996913]\n",
      "0.764\n",
      "loss: [0.06996302]\n",
      "0.766\n",
      "loss: [0.06995935]\n",
      "0.764\n",
      "loss: [0.06995212]\n",
      "0.768\n",
      "loss: [0.06995131]\n",
      "0.762\n",
      "loss: [0.06994372]\n",
      "0.768\n",
      "loss: [0.06994522]\n",
      "0.762\n",
      "loss: [0.06993784]\n",
      "0.768\n",
      "loss: [0.06994125]\n",
      "0.762\n",
      "loss: [0.06993414]\n",
      "0.768\n",
      "loss: [0.0699392]\n",
      "0.762\n",
      "loss: [0.06993189]\n",
      "0.768\n",
      "loss: [0.06993874]\n",
      "0.762\n",
      "loss: [0.06993069]\n",
      "0.77\n",
      "loss: [0.0699404]\n",
      "0.762\n",
      "loss: [0.06993124]\n",
      "0.77\n",
      "loss: [0.06994638]\n",
      "0.762\n",
      "loss: [0.06993632]\n",
      "0.772\n",
      "loss: [0.06996265]\n",
      "0.762\n",
      "loss: [0.06995416]\n",
      "0.778\n",
      "loss: [0.07000607]\n",
      "0.762\n",
      "loss: [0.07001121]\n",
      "0.782\n",
      "loss: [0.07012967]\n",
      "0.762\n",
      "loss: [0.07019919]\n",
      "0.786\n",
      "loss: [0.07051618]\n",
      "0.758\n",
      "loss: [0.07085504]\n",
      "0.794\n",
      "loss: [0.07183569]\n",
      "0.744\n",
      "loss: [0.07327952]\n",
      "0.802\n",
      "loss: [0.07667881]\n",
      "0.736\n",
      "loss: [0.0826695]\n",
      "0.822\n",
      "loss: [0.0952308]\n",
      "0.696\n",
      "loss: [0.11858868]\n",
      "0.86\n",
      "loss: [0.15911359]\n",
      "0.646\n",
      "loss: [0.19852808]\n",
      "0.902\n",
      "loss: [0.16839771]\n",
      "0.642\n",
      "loss: [0.07000252]\n",
      "0.81\n",
      "loss: [0.11354562]\n",
      "0.852\n",
      "loss: [0.10040994]\n",
      "0.698\n",
      "loss: [0.06814048]\n",
      "0.722\n",
      "loss: [0.09823224]\n",
      "0.84\n",
      "loss: [0.05884326]\n",
      "0.778\n",
      "loss: [0.09193393]\n",
      "0.694\n",
      "loss: [0.06105239]\n",
      "0.796\n",
      "loss: [0.08055782]\n",
      "0.82\n",
      "loss: [0.06495319]\n",
      "0.732\n",
      "loss: [0.07408188]\n",
      "0.724\n",
      "loss: [0.06860438]\n",
      "0.814\n",
      "loss: [0.06903176]\n",
      "0.816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.07227469]\n",
      "0.73\n",
      "loss: [0.06638497]\n",
      "0.744\n",
      "loss: [0.07448173]\n",
      "0.82\n",
      "loss: [0.0649619]\n",
      "0.798\n",
      "loss: [0.07693524]\n",
      "0.734\n",
      "loss: [0.06628507]\n",
      "0.786\n",
      "loss: [0.07607035]\n",
      "0.818\n",
      "loss: [0.07105909]\n",
      "0.752\n",
      "loss: [0.07393362]\n",
      "0.74\n",
      "loss: [0.07642149]\n",
      "0.818\n",
      "loss: [0.07191378]\n",
      "0.792\n",
      "loss: [0.07905044]\n",
      "0.74\n",
      "loss: [0.0739629]\n",
      "0.8\n",
      "loss: [0.07668635]\n",
      "0.812\n",
      "loss: [0.0778816]\n",
      "0.754\n",
      "loss: [0.07462599]\n",
      "0.772\n",
      "loss: [0.07835585]\n",
      "0.808\n",
      "loss: [0.0755547]\n",
      "0.766\n",
      "loss: [0.07624422]\n",
      "0.76\n",
      "loss: [0.0772652]\n",
      "0.81\n",
      "loss: [0.07469837]\n",
      "0.782\n",
      "loss: [0.07670898]\n",
      "0.746\n",
      "loss: [0.07547612]\n",
      "0.798\n",
      "loss: [0.07469004]\n",
      "0.79\n",
      "loss: [0.07605293]\n",
      "0.748\n",
      "loss: [0.07421582]\n",
      "0.79\n",
      "loss: [0.07473321]\n",
      "0.796\n",
      "loss: [0.07478479]\n",
      "0.756\n",
      "loss: [0.07363026]\n",
      "0.772\n",
      "loss: [0.07432874]\n",
      "0.798\n",
      "loss: [0.07361154]\n",
      "0.76\n",
      "loss: [0.07337723]\n",
      "0.76\n",
      "loss: [0.07366897]\n",
      "0.798\n",
      "loss: [0.07291725]\n",
      "0.766\n",
      "loss: [0.07318509]\n",
      "0.756\n",
      "loss: [0.07304744]\n",
      "0.792\n",
      "loss: [0.07254867]\n",
      "0.772\n",
      "loss: [0.07290704]\n",
      "0.758\n",
      "loss: [0.07254437]\n",
      "0.788\n",
      "loss: [0.07229038]\n",
      "0.78\n",
      "loss: [0.07254124]\n",
      "0.76\n",
      "loss: [0.07212055]\n",
      "0.78\n",
      "loss: [0.07200398]\n",
      "0.78\n",
      "loss: [0.07214445]\n",
      "0.762\n",
      "loss: [0.07177465]\n",
      "0.78\n",
      "loss: [0.07168167]\n",
      "0.78\n",
      "loss: [0.07177035]\n",
      "0.762\n",
      "loss: [0.07147419]\n",
      "0.78\n",
      "loss: [0.07139859]\n",
      "0.78\n",
      "loss: [0.07145182]\n",
      "0.762\n",
      "loss: [0.07121663]\n",
      "0.776\n",
      "loss: [0.07115517]\n",
      "0.778\n",
      "loss: [0.07118006]\n",
      "0.762\n",
      "loss: [0.07098794]\n",
      "0.776\n",
      "loss: [0.07093295]\n",
      "0.78\n",
      "loss: [0.07093993]\n",
      "0.762\n",
      "loss: [0.07078474]\n",
      "0.774\n",
      "loss: [0.07073604]\n",
      "0.776\n",
      "loss: [0.07073993]\n",
      "0.762\n",
      "loss: [0.07061917]\n",
      "0.772\n",
      "loss: [0.07057768]\n",
      "0.776\n",
      "loss: [0.07058355]\n",
      "0.762\n",
      "loss: [0.07048985]\n",
      "0.772\n",
      "loss: [0.07045083]\n",
      "0.772\n",
      "loss: [0.07045272]\n",
      "0.762\n",
      "loss: [0.07037462]\n",
      "0.772\n",
      "loss: [0.07033385]\n",
      "0.77\n",
      "loss: [0.07033024]\n",
      "0.762\n",
      "loss: [0.07026748]\n",
      "0.772\n",
      "loss: [0.07023177]\n",
      "0.768\n",
      "loss: [0.0702286]\n",
      "0.762\n",
      "loss: [0.070185]\n",
      "0.772\n",
      "loss: [0.07016005]\n",
      "0.768\n",
      "loss: [0.07015829]\n",
      "0.762\n",
      "loss: [0.07013027]\n",
      "0.772\n",
      "loss: [0.07011347]\n",
      "0.764\n",
      "loss: [0.070108]\n",
      "0.762\n",
      "loss: [0.07008829]\n",
      "0.772\n",
      "loss: [0.07007701]\n",
      "0.764\n",
      "loss: [0.07006758]\n",
      "0.764\n",
      "loss: [0.07005542]\n",
      "0.772\n",
      "loss: [0.07005224]\n",
      "0.762\n",
      "loss: [0.07004298]\n",
      "0.764\n",
      "loss: [0.07003789]\n",
      "0.77\n",
      "loss: [0.07004144]\n",
      "0.762\n",
      "loss: [0.07003215]\n",
      "0.764\n",
      "loss: [0.07002895]\n",
      "0.77\n",
      "loss: [0.07003276]\n",
      "0.762\n",
      "loss: [0.07002113]\n",
      "0.768\n",
      "loss: [0.070016]\n",
      "0.768\n",
      "loss: [0.07001644]\n",
      "0.762\n",
      "loss: [0.07000441]\n",
      "0.768\n",
      "loss: [0.06999934]\n",
      "0.764\n",
      "loss: [0.06999727]\n",
      "0.762\n",
      "loss: [0.06998766]\n",
      "0.768\n",
      "loss: [0.06998421]\n",
      "0.762\n",
      "loss: [0.06997932]\n",
      "0.762\n",
      "loss: [0.06997105]\n",
      "0.768\n",
      "loss: [0.06996782]\n",
      "0.762\n",
      "loss: [0.06995899]\n",
      "0.764\n",
      "loss: [0.06995091]\n",
      "0.768\n",
      "loss: [0.06994723]\n",
      "0.762\n",
      "loss: [0.06993735]\n",
      "0.768\n",
      "loss: [0.06993123]\n",
      "0.766\n",
      "loss: [0.06992821]\n",
      "0.762\n",
      "loss: [0.06992056]\n",
      "0.768\n",
      "loss: [0.06991748]\n",
      "0.764\n",
      "loss: [0.06991464]\n",
      "0.764\n",
      "loss: [0.06990966]\n",
      "0.768\n",
      "loss: [0.06990866]\n",
      "0.764\n",
      "loss: [0.06990526]\n",
      "0.764\n",
      "loss: [0.06990229]\n",
      "0.768\n",
      "loss: [0.06990235]\n",
      "0.764\n",
      "loss: [0.06989929]\n",
      "0.766\n",
      "loss: [0.06989856]\n",
      "0.764\n",
      "loss: [0.06989944]\n",
      "0.764\n",
      "loss: [0.06989799]\n",
      "0.768\n",
      "loss: [0.06989934]\n",
      "0.764\n",
      "loss: [0.06990028]\n",
      "0.764\n",
      "loss: [0.06990026]\n",
      "0.766\n",
      "loss: [0.06990249]\n",
      "0.764\n",
      "loss: [0.06990285]\n",
      "0.764\n",
      "loss: [0.06990373]\n",
      "0.764\n",
      "loss: [0.0699059]\n",
      "0.764\n",
      "loss: [0.06990612]\n",
      "0.764\n",
      "loss: [0.06990782]\n",
      "0.764\n",
      "loss: [0.06990957]\n",
      "0.764\n",
      "loss: [0.06991023]\n",
      "0.764\n",
      "loss: [0.06991244]\n",
      "0.764\n",
      "loss: [0.06991351]\n",
      "0.764\n",
      "loss: [0.06991457]\n",
      "0.764\n",
      "loss: [0.06991647]\n",
      "0.764\n",
      "loss: [0.06991687]\n",
      "0.764\n",
      "loss: [0.06991804]\n",
      "0.764\n",
      "loss: [0.06991907]\n",
      "0.764\n",
      "loss: [0.06991921]\n",
      "0.764\n",
      "loss: [0.06992028]\n",
      "0.764\n",
      "loss: [0.0699205]\n",
      "0.764\n",
      "loss: [0.0699207]\n",
      "0.764\n",
      "loss: [0.06992143]\n",
      "0.764\n",
      "loss: [0.06992118]\n",
      "0.764\n",
      "loss: [0.06992152]\n",
      "0.764\n",
      "loss: [0.06992166]\n",
      "0.764\n",
      "loss: [0.06992131]\n",
      "0.764\n",
      "loss: [0.06992161]\n",
      "0.764\n",
      "loss: [0.06992124]\n",
      "0.764\n",
      "loss: [0.06992109]\n",
      "0.764\n",
      "loss: [0.06992118]\n",
      "0.764\n",
      "loss: [0.06992076]\n",
      "0.764\n",
      "loss: [0.06992091]\n",
      "0.764\n",
      "loss: [0.06992082]\n",
      "0.764\n",
      "loss: [0.06992072]\n",
      "0.764\n",
      "loss: [0.06992105]\n",
      "0.764\n",
      "loss: [0.06992093]\n",
      "0.764\n",
      "loss: [0.06992123]\n",
      "0.764\n",
      "loss: [0.06992149]\n",
      "0.764\n",
      "loss: [0.06992159]\n",
      "0.764\n",
      "loss: [0.0699221]\n",
      "0.764\n",
      "loss: [0.06992226]\n",
      "0.764\n",
      "loss: [0.06992264]\n",
      "0.764\n",
      "loss: [0.06992308]\n",
      "0.764\n",
      "loss: [0.0699233]\n",
      "0.764\n",
      "loss: [0.06992381]\n",
      "0.764\n",
      "loss: [0.06992409]\n",
      "0.764\n",
      "loss: [0.06992443]\n",
      "0.764\n",
      "loss: [0.06992482]\n",
      "0.764\n",
      "loss: [0.06992498]\n",
      "0.764\n",
      "loss: [0.06992532]\n",
      "0.764\n",
      "loss: [0.06992544]\n",
      "0.764\n",
      "loss: [0.06992557]\n",
      "0.764\n",
      "loss: [0.06992575]\n",
      "0.764\n",
      "loss: [0.06992572]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992579]\n",
      "0.764\n",
      "loss: [0.06992582]\n",
      "0.764\n",
      "loss: [0.0699257]\n",
      "0.764\n",
      "loss: [0.0699257]\n",
      "0.764\n",
      "loss: [0.06992561]\n",
      "0.764\n",
      "loss: [0.06992552]\n",
      "0.764\n",
      "loss: [0.06992549]\n",
      "0.764\n",
      "loss: [0.06992536]\n",
      "0.764\n",
      "loss: [0.06992538]\n",
      "0.764\n",
      "loss: [0.06992535]\n",
      "0.764\n",
      "loss: [0.06992538]\n",
      "0.764\n",
      "loss: [0.06992548]\n",
      "0.764\n",
      "loss: [0.06992552]\n",
      "0.764\n",
      "loss: [0.06992564]\n",
      "0.764\n",
      "loss: [0.0699257]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992588]\n",
      "0.764\n",
      "loss: [0.06992593]\n",
      "0.764\n",
      "loss: [0.069926]\n",
      "0.764\n",
      "loss: [0.06992602]\n",
      "0.764\n",
      "loss: [0.06992608]\n",
      "0.764\n",
      "loss: [0.06992614]\n",
      "0.764\n",
      "loss: [0.06992617]\n",
      "0.764\n",
      "loss: [0.06992621]\n",
      "0.764\n",
      "loss: [0.0699262]\n",
      "0.764\n",
      "loss: [0.06992622]\n",
      "0.764\n",
      "loss: [0.06992619]\n",
      "0.764\n",
      "loss: [0.06992619]\n",
      "0.764\n",
      "loss: [0.06992618]\n",
      "0.764\n",
      "loss: [0.06992615]\n",
      "0.764\n",
      "loss: [0.06992614]\n",
      "0.764\n",
      "loss: [0.06992608]\n",
      "0.764\n",
      "loss: [0.06992608]\n",
      "0.764\n",
      "loss: [0.06992602]\n",
      "0.764\n",
      "loss: [0.06992598]\n",
      "0.764\n",
      "loss: [0.06992592]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.06992583]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992586]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992586]\n",
      "0.764\n",
      "loss: [0.06992589]\n",
      "0.764\n",
      "loss: [0.06992588]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992589]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.0699258]\n",
      "0.764\n",
      "loss: [0.0699258]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992583]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992602]\n",
      "0.764\n",
      "loss: [0.069926]\n",
      "0.764\n",
      "loss: [0.06992602]\n",
      "0.764\n",
      "loss: [0.069926]\n",
      "0.764\n",
      "loss: [0.069926]\n",
      "0.764\n",
      "loss: [0.06992597]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992594]\n",
      "0.764\n",
      "loss: [0.06992594]\n",
      "0.764\n",
      "loss: [0.06992591]\n",
      "0.764\n",
      "loss: [0.06992593]\n",
      "0.764\n",
      "loss: [0.06992589]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992586]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992588]\n",
      "0.764\n",
      "loss: [0.06992579]\n",
      "0.764\n",
      "loss: [0.06992586]\n",
      "0.764\n",
      "loss: [0.06992572]\n",
      "0.764\n",
      "loss: [0.06992585]\n",
      "0.764\n",
      "loss: [0.06992566]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992561]\n",
      "0.764\n",
      "loss: [0.06992593]\n",
      "0.764\n",
      "loss: [0.06992553]\n",
      "0.764\n",
      "loss: [0.06992602]\n",
      "0.764\n",
      "loss: [0.06992541]\n",
      "0.764\n",
      "loss: [0.06992617]\n",
      "0.764\n",
      "loss: [0.06992525]\n",
      "0.768\n",
      "loss: [0.06992646]\n",
      "0.764\n",
      "loss: [0.06992503]\n",
      "0.768\n",
      "loss: [0.069927]\n",
      "0.764\n",
      "loss: [0.06992478]\n",
      "0.768\n",
      "loss: [0.06992815]\n",
      "0.762\n",
      "loss: [0.06992468]\n",
      "0.768\n",
      "loss: [0.06993072]\n",
      "0.762\n",
      "loss: [0.06992555]\n",
      "0.768\n",
      "loss: [0.06993718]\n",
      "0.762\n",
      "loss: [0.06993044]\n",
      "0.772\n",
      "loss: [0.06995524]\n",
      "0.762\n",
      "loss: [0.06995106]\n",
      "0.776\n",
      "loss: [0.07001079]\n",
      "0.762\n",
      "loss: [0.07003151]\n",
      "0.782\n",
      "loss: [0.07019686]\n",
      "0.76\n",
      "loss: [0.07034311]\n",
      "0.788\n",
      "loss: [0.07086761]\n",
      "0.754\n",
      "loss: [0.07157786]\n",
      "0.802\n",
      "loss: [0.07345097]\n",
      "0.74\n",
      "loss: [0.07668603]\n",
      "0.816\n",
      "loss: [0.08420725]\n",
      "0.722\n",
      "loss: [0.09972575]\n",
      "0.842\n",
      "loss: [0.13626227]\n",
      "0.652\n",
      "loss: [0.22449204]\n",
      "0.91\n",
      "loss: [0.44375026]\n",
      "0.562\n",
      "loss: [0.5597628]\n",
      "0.948\n",
      "loss: [0.1657209]\n",
      "0.666\n",
      "loss: [0.295455]\n",
      "0.626\n",
      "loss: [0.1564431]\n",
      "0.866\n",
      "loss: [0.2942902]\n",
      "0.904\n",
      "loss: [0.07320587]\n",
      "0.73\n",
      "loss: [0.29867828]\n",
      "0.612\n",
      "loss: [0.07549769]\n",
      "0.722\n",
      "loss: [0.1495389]\n",
      "0.848\n",
      "loss: [0.17622833]\n",
      "0.856\n",
      "loss: [0.06076339]\n",
      "0.766\n",
      "loss: [0.15072851]\n",
      "0.656\n",
      "loss: [0.12716725]\n",
      "0.672\n",
      "loss: [0.06345242]\n",
      "0.782\n",
      "loss: [0.12651768]\n",
      "0.85\n",
      "loss: [0.10867432]\n",
      "0.838\n",
      "loss: [0.06512487]\n",
      "0.778\n",
      "loss: [0.11082492]\n",
      "0.714\n",
      "loss: [0.10366197]\n",
      "0.718\n",
      "loss: [0.06722925]\n",
      "0.794\n",
      "loss: [0.09850468]\n",
      "0.844\n",
      "loss: [0.09629221]\n",
      "0.842\n",
      "loss: [0.069593]\n",
      "0.786\n",
      "loss: [0.09628245]\n",
      "0.726\n",
      "loss: [0.09205316]\n",
      "0.728\n",
      "loss: [0.07357701]\n",
      "0.802\n",
      "loss: [0.09663705]\n",
      "0.84\n",
      "loss: [0.08583901]\n",
      "0.824\n",
      "loss: [0.08126808]\n",
      "0.758\n",
      "loss: [0.09902346]\n",
      "0.73\n",
      "loss: [0.08240638]\n",
      "0.77\n",
      "loss: [0.09120126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832\n",
      "loss: [0.09329508]\n",
      "0.834\n",
      "loss: [0.08450698]\n",
      "0.788\n",
      "loss: [0.09730121]\n",
      "0.742\n",
      "loss: [0.08600118]\n",
      "0.784\n",
      "loss: [0.0917638]\n",
      "0.832\n",
      "loss: [0.09005279]\n",
      "0.828\n",
      "loss: [0.08659053]\n",
      "0.772\n",
      "loss: [0.09209425]\n",
      "0.754\n",
      "loss: [0.08396117]\n",
      "0.8\n",
      "loss: [0.08996191]\n",
      "0.832\n",
      "loss: [0.08359091]\n",
      "0.804\n",
      "loss: [0.08716]\n",
      "0.754\n",
      "loss: [0.08400904]\n",
      "0.772\n",
      "loss: [0.08385978]\n",
      "0.818\n",
      "loss: [0.08373364]\n",
      "0.816\n",
      "loss: [0.08166201]\n",
      "0.782\n",
      "loss: [0.08322074]\n",
      "0.762\n",
      "loss: [0.07980788]\n",
      "0.806\n",
      "loss: [0.0816971]\n",
      "0.812\n",
      "loss: [0.07851838]\n",
      "0.792\n",
      "loss: [0.08042811]\n",
      "0.758\n",
      "loss: [0.07751428]\n",
      "0.792\n",
      "loss: [0.07881822]\n",
      "0.812\n",
      "loss: [0.0766957]\n",
      "0.798\n",
      "loss: [0.07777913]\n",
      "0.762\n",
      "loss: [0.07609016]\n",
      "0.78\n",
      "loss: [0.07666562]\n",
      "0.806\n",
      "loss: [0.07549434]\n",
      "0.8\n",
      "loss: [0.07602218]\n",
      "0.762\n",
      "loss: [0.07503488]\n",
      "0.776\n",
      "loss: [0.07524817]\n",
      "0.804\n",
      "loss: [0.07446767]\n",
      "0.802\n",
      "loss: [0.0747511]\n",
      "0.762\n",
      "loss: [0.07400915]\n",
      "0.772\n",
      "loss: [0.074141]\n",
      "0.802\n",
      "loss: [0.0735239]\n",
      "0.794\n",
      "loss: [0.07376973]\n",
      "0.762\n",
      "loss: [0.07314657]\n",
      "0.77\n",
      "loss: [0.07328007]\n",
      "0.798\n",
      "loss: [0.07276452]\n",
      "0.782\n",
      "loss: [0.07295188]\n",
      "0.766\n",
      "loss: [0.07242156]\n",
      "0.776\n",
      "loss: [0.07248565]\n",
      "0.798\n",
      "loss: [0.07207825]\n",
      "0.776\n",
      "loss: [0.07213109]\n",
      "0.766\n",
      "loss: [0.07173274]\n",
      "0.78\n",
      "loss: [0.07169137]\n",
      "0.788\n",
      "loss: [0.0714522]\n",
      "0.766\n",
      "loss: [0.07138828]\n",
      "0.766\n",
      "loss: [0.0711784]\n",
      "0.784\n",
      "loss: [0.07107065]\n",
      "0.784\n",
      "loss: [0.07099633]\n",
      "0.766\n",
      "loss: [0.07086028]\n",
      "0.766\n",
      "loss: [0.07078623]\n",
      "0.786\n",
      "loss: [0.07065142]\n",
      "0.774\n",
      "loss: [0.07064949]\n",
      "0.764\n",
      "loss: [0.07049651]\n",
      "0.768\n",
      "loss: [0.07047068]\n",
      "0.784\n",
      "loss: [0.07036736]\n",
      "0.766\n",
      "loss: [0.07036813]\n",
      "0.762\n",
      "loss: [0.07026805]\n",
      "0.772\n",
      "loss: [0.07024782]\n",
      "0.776\n",
      "loss: [0.07021355]\n",
      "0.762\n",
      "loss: [0.07018871]\n",
      "0.762\n",
      "loss: [0.07014977]\n",
      "0.774\n",
      "loss: [0.07011524]\n",
      "0.772\n",
      "loss: [0.07011526]\n",
      "0.762\n",
      "loss: [0.07006405]\n",
      "0.762\n",
      "loss: [0.07004626]\n",
      "0.772\n",
      "loss: [0.07001038]\n",
      "0.762\n",
      "loss: [0.07000729]\n",
      "0.762\n",
      "loss: [0.0699646]\n",
      "0.766\n",
      "loss: [0.06995086]\n",
      "0.768\n",
      "loss: [0.06993959]\n",
      "0.762\n",
      "loss: [0.06992687]\n",
      "0.762\n",
      "loss: [0.06990992]\n",
      "0.768\n",
      "loss: [0.06989761]\n",
      "0.764\n",
      "loss: [0.06990073]\n",
      "0.762\n",
      "loss: [0.06988113]\n",
      "0.764\n",
      "loss: [0.06987402]\n",
      "0.768\n",
      "loss: [0.06986567]\n",
      "0.764\n",
      "loss: [0.06986529]\n",
      "0.762\n",
      "loss: [0.06985164]\n",
      "0.768\n",
      "loss: [0.06984842]\n",
      "0.768\n",
      "loss: [0.06985191]\n",
      "0.762\n",
      "loss: [0.06984964]\n",
      "0.762\n",
      "loss: [0.069849]\n",
      "0.768\n",
      "loss: [0.06985167]\n",
      "0.764\n",
      "loss: [0.06986044]\n",
      "0.762\n",
      "loss: [0.06985866]\n",
      "0.764\n",
      "loss: [0.06986201]\n",
      "0.768\n",
      "loss: [0.06986752]\n",
      "0.762\n",
      "loss: [0.06987154]\n",
      "0.762\n",
      "loss: [0.0698716]\n",
      "0.766\n",
      "loss: [0.0698752]\n",
      "0.764\n",
      "loss: [0.06988244]\n",
      "0.762\n",
      "loss: [0.06988404]\n",
      "0.762\n",
      "loss: [0.0698875]\n",
      "0.766\n",
      "loss: [0.06989311]\n",
      "0.762\n",
      "loss: [0.06989872]\n",
      "0.762\n",
      "loss: [0.06989982]\n",
      "0.764\n",
      "loss: [0.06990273]\n",
      "0.764\n",
      "loss: [0.06990699]\n",
      "0.762\n",
      "loss: [0.06990741]\n",
      "0.762\n",
      "loss: [0.06990701]\n",
      "0.764\n",
      "loss: [0.06990809]\n",
      "0.764\n",
      "loss: [0.06990948]\n",
      "0.762\n",
      "loss: [0.06990774]\n",
      "0.764\n",
      "loss: [0.06990744]\n",
      "0.764\n",
      "loss: [0.06990869]\n",
      "0.762\n",
      "loss: [0.0699082]\n",
      "0.762\n",
      "loss: [0.06990685]\n",
      "0.764\n",
      "loss: [0.06990686]\n",
      "0.764\n",
      "loss: [0.06990722]\n",
      "0.762\n",
      "loss: [0.06990538]\n",
      "0.764\n",
      "loss: [0.06990425]\n",
      "0.764\n",
      "loss: [0.06990436]\n",
      "0.764\n",
      "loss: [0.06990366]\n",
      "0.764\n",
      "loss: [0.06990242]\n",
      "0.764\n",
      "loss: [0.06990261]\n",
      "0.764\n",
      "loss: [0.06990352]\n",
      "0.764\n",
      "loss: [0.06990335]\n",
      "0.764\n",
      "loss: [0.06990378]\n",
      "0.764\n",
      "loss: [0.06990529]\n",
      "0.764\n",
      "loss: [0.06990627]\n",
      "0.764\n",
      "loss: [0.06990666]\n",
      "0.764\n",
      "loss: [0.06990787]\n",
      "0.764\n",
      "loss: [0.06990937]\n",
      "0.764\n",
      "loss: [0.06990997]\n",
      "0.764\n",
      "loss: [0.0699108]\n",
      "0.764\n",
      "loss: [0.06991237]\n",
      "0.764\n",
      "loss: [0.06991358]\n",
      "0.764\n",
      "loss: [0.06991436]\n",
      "0.764\n",
      "loss: [0.0699157]\n",
      "0.764\n",
      "loss: [0.06991721]\n",
      "0.764\n",
      "loss: [0.06991801]\n",
      "0.764\n",
      "loss: [0.06991887]\n",
      "0.764\n",
      "loss: [0.06992012]\n",
      "0.764\n",
      "loss: [0.06992095]\n",
      "0.764\n",
      "loss: [0.06992141]\n",
      "0.764\n",
      "loss: [0.06992223]\n",
      "0.764\n",
      "loss: [0.06992314]\n",
      "0.764\n",
      "loss: [0.06992359]\n",
      "0.764\n",
      "loss: [0.06992411]\n",
      "0.764\n",
      "loss: [0.06992494]\n",
      "0.764\n",
      "loss: [0.06992549]\n",
      "0.764\n",
      "loss: [0.06992579]\n",
      "0.764\n",
      "loss: [0.06992631]\n",
      "0.764\n",
      "loss: [0.06992683]\n",
      "0.764\n",
      "loss: [0.06992697]\n",
      "0.764\n",
      "loss: [0.06992718]\n",
      "0.764\n",
      "loss: [0.06992757]\n",
      "0.764\n",
      "loss: [0.06992773]\n",
      "0.764\n",
      "loss: [0.06992779]\n",
      "0.764\n",
      "loss: [0.06992807]\n",
      "0.764\n",
      "loss: [0.06992833]\n",
      "0.764\n",
      "loss: [0.06992838]\n",
      "0.764\n",
      "loss: [0.06992851]\n",
      "0.764\n",
      "loss: [0.06992874]\n",
      "0.764\n",
      "loss: [0.06992876]\n",
      "0.764\n",
      "loss: [0.06992872]\n",
      "0.764\n",
      "loss: [0.06992882]\n",
      "0.764\n",
      "loss: [0.06992883]\n",
      "0.764\n",
      "loss: [0.06992873]\n",
      "0.764\n",
      "loss: [0.06992872]\n",
      "0.764\n",
      "loss: [0.06992877]\n",
      "0.764\n",
      "loss: [0.06992868]\n",
      "0.764\n",
      "loss: [0.0699286]\n",
      "0.764\n",
      "loss: [0.06992862]\n",
      "0.764\n",
      "loss: [0.06992856]\n",
      "0.764\n",
      "loss: [0.06992844]\n",
      "0.764\n",
      "loss: [0.06992841]\n",
      "0.764\n",
      "loss: [0.06992838]\n",
      "0.764\n",
      "loss: [0.06992826]\n",
      "0.764\n",
      "loss: [0.06992818]\n",
      "0.764\n",
      "loss: [0.06992816]\n",
      "0.764\n",
      "loss: [0.0699281]\n",
      "0.764\n",
      "loss: [0.06992804]\n",
      "0.764\n",
      "loss: [0.06992804]\n",
      "0.764\n",
      "loss: [0.06992805]\n",
      "0.764\n",
      "loss: [0.06992802]\n",
      "0.764\n",
      "loss: [0.06992804]\n",
      "0.764\n",
      "loss: [0.06992809]\n",
      "0.764\n",
      "loss: [0.0699281]\n",
      "0.764\n",
      "loss: [0.06992811]\n",
      "0.764\n",
      "loss: [0.06992817]\n",
      "0.764\n",
      "loss: [0.06992819]\n",
      "0.764\n",
      "loss: [0.06992821]\n",
      "0.764\n",
      "loss: [0.06992824]\n",
      "0.764\n",
      "loss: [0.06992827]\n",
      "0.764\n",
      "loss: [0.06992827]\n",
      "0.764\n",
      "loss: [0.06992827]\n",
      "0.764\n",
      "loss: [0.06992828]\n",
      "0.764\n",
      "loss: [0.06992826]\n",
      "0.764\n",
      "loss: [0.06992822]\n",
      "0.764\n",
      "loss: [0.0699282]\n",
      "0.764\n",
      "loss: [0.06992815]\n",
      "0.764\n",
      "loss: [0.06992809]\n",
      "0.764\n",
      "loss: [0.06992805]\n",
      "0.764\n",
      "loss: [0.069928]\n",
      "0.764\n",
      "loss: [0.06992794]\n",
      "0.764\n",
      "loss: [0.06992789]\n",
      "0.764\n",
      "loss: [0.06992785]\n",
      "0.764\n",
      "loss: [0.0699278]\n",
      "0.764\n",
      "loss: [0.06992776]\n",
      "0.764\n",
      "loss: [0.06992773]\n",
      "0.764\n",
      "loss: [0.06992768]\n",
      "0.764\n",
      "loss: [0.06992766]\n",
      "0.764\n",
      "loss: [0.06992765]\n",
      "0.764\n",
      "loss: [0.06992764]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992764]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992764]\n",
      "0.764\n",
      "loss: [0.06992765]\n",
      "0.764\n",
      "loss: [0.06992765]\n",
      "0.764\n",
      "loss: [0.06992765]\n",
      "0.764\n",
      "loss: [0.06992766]\n",
      "0.764\n",
      "loss: [0.06992765]\n",
      "0.764\n",
      "loss: [0.06992766]\n",
      "0.764\n",
      "loss: [0.06992765]\n",
      "0.764\n",
      "loss: [0.06992763]\n",
      "0.764\n",
      "loss: [0.06992761]\n",
      "0.764\n",
      "loss: [0.06992757]\n",
      "0.764\n",
      "loss: [0.06992753]\n",
      "0.764\n",
      "loss: [0.06992746]\n",
      "0.764\n",
      "loss: [0.06992739]\n",
      "0.764\n",
      "loss: [0.0699273]\n",
      "0.764\n",
      "loss: [0.06992722]\n",
      "0.764\n",
      "loss: [0.06992711]\n",
      "0.764\n",
      "loss: [0.06992699]\n",
      "0.764\n",
      "loss: [0.0699269]\n",
      "0.764\n",
      "loss: [0.06992679]\n",
      "0.764\n",
      "loss: [0.06992668]\n",
      "0.764\n",
      "loss: [0.06992659]\n",
      "0.764\n",
      "loss: [0.0699265]\n",
      "0.764\n",
      "loss: [0.06992643]\n",
      "0.764\n",
      "loss: [0.06992636]\n",
      "0.764\n",
      "loss: [0.0699263]\n",
      "0.764\n",
      "loss: [0.06992625]\n",
      "0.764\n",
      "loss: [0.0699262]\n",
      "0.764\n",
      "loss: [0.06992614]\n",
      "0.764\n",
      "loss: [0.06992609]\n",
      "0.764\n",
      "loss: [0.06992605]\n",
      "0.764\n",
      "loss: [0.06992602]\n",
      "0.764\n",
      "loss: [0.06992599]\n",
      "0.764\n",
      "loss: [0.06992598]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992597]\n",
      "0.764\n",
      "loss: [0.06992599]\n",
      "0.764\n",
      "loss: [0.06992599]\n",
      "0.764\n",
      "loss: [0.06992599]\n",
      "0.764\n",
      "loss: [0.06992598]\n",
      "0.764\n",
      "loss: [0.06992596]\n",
      "0.764\n",
      "loss: [0.06992594]\n",
      "0.764\n",
      "loss: [0.06992592]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992588]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992595]\n",
      "0.764\n",
      "loss: [0.069926]\n",
      "0.764\n",
      "loss: [0.06992605]\n",
      "0.764\n",
      "loss: [0.0699261]\n",
      "0.764\n",
      "loss: [0.06992614]\n",
      "0.764\n",
      "loss: [0.06992616]\n",
      "0.764\n",
      "loss: [0.06992617]\n",
      "0.764\n",
      "loss: [0.06992615]\n",
      "0.764\n",
      "loss: [0.0699261]\n",
      "0.764\n",
      "loss: [0.06992605]\n",
      "0.764\n",
      "loss: [0.06992598]\n",
      "0.764\n",
      "loss: [0.06992589]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992573]\n",
      "0.764\n",
      "loss: [0.06992565]\n",
      "0.764\n",
      "loss: [0.06992561]\n",
      "0.764\n",
      "loss: [0.06992557]\n",
      "0.764\n",
      "loss: [0.06992555]\n",
      "0.764\n",
      "loss: [0.06992556]\n",
      "0.764\n",
      "loss: [0.06992558]\n",
      "0.764\n",
      "loss: [0.06992561]\n",
      "0.764\n",
      "loss: [0.06992565]\n",
      "0.764\n",
      "loss: [0.0699257]\n",
      "0.764\n",
      "loss: [0.06992576]\n",
      "0.764\n",
      "loss: [0.06992582]\n",
      "0.764\n",
      "loss: [0.06992586]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992592]\n",
      "0.764\n",
      "loss: [0.06992591]\n",
      "0.764\n",
      "loss: [0.06992592]\n",
      "0.764\n",
      "loss: [0.0699259]\n",
      "0.764\n",
      "loss: [0.06992589]\n",
      "0.764\n",
      "loss: [0.06992587]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.0699258]\n",
      "0.764\n",
      "loss: [0.06992579]\n",
      "0.764\n",
      "loss: [0.06992577]\n",
      "0.764\n",
      "loss: [0.06992577]\n",
      "0.764\n",
      "loss: [0.06992576]\n",
      "0.764\n",
      "loss: [0.06992575]\n",
      "0.764\n",
      "loss: [0.06992576]\n",
      "0.764\n",
      "loss: [0.06992577]\n",
      "0.764\n",
      "loss: [0.06992579]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992582]\n",
      "0.764\n",
      "loss: [0.06992583]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992584]\n",
      "0.764\n",
      "loss: [0.06992583]\n",
      "0.764\n",
      "loss: [0.06992582]\n",
      "0.764\n",
      "loss: [0.06992581]\n",
      "0.764\n",
      "loss: [0.06992578]\n",
      "0.764\n",
      "loss: [0.06992576]\n",
      "0.764\n",
      "loss: [0.06992576]\n",
      "0.764\n",
      "loss: [0.06992573]\n",
      "0.764\n",
      "loss: [0.06992573]\n",
      "0.764\n"
     ]
    }
   ],
   "source": [
    "n = g.number_of_nodes()\n",
    "n_hidden = 32\n",
    "n_embedding_updates = 1\n",
    "n_parameter_updates = 1\n",
    "alpha = 0.1\n",
    "batch_size = 64\n",
    "lr = 1e-1\n",
    "\n",
    "g.ndata['x'] = mx.nd.array([conv(i) for i in range(n)])\n",
    "#g.ndata['x'] = mx.nd.arange(n).reshape(n, 1)\n",
    "g.ndata['h'] = mx.nd.random.normal(shape=(n, n_hidden))\n",
    "\n",
    "steady_state_operator = SteadyStateOperator(n_hidden, 'relu')\n",
    "steady_state_operator.initialize()\n",
    "predictor = Predictor(n_hidden, 'relu')\n",
    "predictor.initialize()\n",
    "trainer = gluon.Trainer(predictor.collect_params(), 'adam',\n",
    "                        {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "def update_parameters(g, trainer):\n",
    "    with mx.autograd.record():\n",
    "        steady_state_operator(g)\n",
    "        z = predictor(g)\n",
    "        loss = loss_fcn(z, labels, mx.nd.expand_dims(train_mask, 1))\n",
    "        loss = loss.sum() / n_train_samples\n",
    "        print(\"loss: \" + str(loss.asnumpy()))\n",
    "    loss.backward()\n",
    "    trainer.step(1)  # divide gradients by the number of labelled nodes\n",
    "    return loss.asnumpy()[0]\n",
    "\n",
    "def train(g, trainer):\n",
    "     # first phase\n",
    "    for i in range(n_embedding_updates):\n",
    "        update_embeddings(g, steady_state_operator)\n",
    "    # second phase\n",
    "    for i in range(n_parameter_updates):\n",
    "        loss = update_parameters(g, trainer)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train(g, trainer)\n",
    "    acc = evaluate(g.ndata['z'], labels, eval_mask)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
